{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_format1.csv')\n",
    "train_data = pd.read_csv('../data/train_format1.csv')\n",
    "\n",
    "user_info = pd.read_csv('../data/user_info_format1.csv')\n",
    "user_log = pd.read_csv('../data/user_log_format1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>376517</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>234512</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344532</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186135</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30230</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  age_range  gender\n",
       "0   376517        6.0     1.0\n",
       "1   234512        5.0     0.0\n",
       "2   344532        5.0     0.0\n",
       "3   186135        5.0     0.0\n",
       "4    30230        5.0     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>action_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>328862</td>\n",
       "      <td>323294</td>\n",
       "      <td>833</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>328862</td>\n",
       "      <td>844400</td>\n",
       "      <td>1271</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328862</td>\n",
       "      <td>575153</td>\n",
       "      <td>1271</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328862</td>\n",
       "      <td>996875</td>\n",
       "      <td>1271</td>\n",
       "      <td>2882</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>328862</td>\n",
       "      <td>1086186</td>\n",
       "      <td>1271</td>\n",
       "      <td>1253</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type\n",
       "0   328862   323294     833       2882    2661.0         829            0\n",
       "1   328862   844400    1271       2882    2661.0         829            0\n",
       "2   328862   575153    1271       2882    2661.0         829            0\n",
       "3   328862   996875    1271       2882    2661.0         829            0\n",
       "4   328862  1086186    1271       1253    1049.0         829            0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label\n",
       "0    34176         3906      0\n",
       "1    34176          121      0\n",
       "2    34176         4356      1\n",
       "3    34176         2217      0\n",
       "4   230784         4818      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163968</td>\n",
       "      <td>4605</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>360576</td>\n",
       "      <td>1581</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98688</td>\n",
       "      <td>1964</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98688</td>\n",
       "      <td>3645</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295296</td>\n",
       "      <td>3361</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  prob\n",
       "0   163968         4605   NaN\n",
       "1   360576         1581   NaN\n",
       "2    98688         1964   NaN\n",
       "3    98688         3645   NaN\n",
       "4   295296         3361   NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = train_data.append(test_data) # 全数据 = 合并训练集和测试集数据\n",
    "all_data = all_data.merge(user_info,on=['user_id'],how='left') # 全数据 = 合并全输就和用户画像\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  prob  age_range  gender\n",
       "0    34176         3906    0.0   NaN        6.0     0.0\n",
       "1    34176          121    0.0   NaN        6.0     0.0\n",
       "2    34176         4356    1.0   NaN        6.0     0.0\n",
       "3    34176         2217    0.0   NaN        6.0     0.0\n",
       "4   230784         4818    0.0   NaN        0.0     0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照用户id与时间戳排序\n",
    "\n",
    "user_log = user_log.sort_values(['user_id','time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "合并日志文件 \n",
    "1. 首先按照用户id分组\n",
    "2. 正对其余列 同个id的数据转化为字符串相加\n",
    "'''\n",
    "\n",
    "# 字符串相加函数\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "# 合并字典\n",
    "agg_dict = {\n",
    "            'item_id' : list_join_func,\t\n",
    "            'cat_id' : list_join_func,\n",
    "            'seller_id' : list_join_func,\n",
    "            'brand_id' : list_join_func,\n",
    "            'time_stamp' : list_join_func,\n",
    "            'action_type' : list_join_func\n",
    "        }\n",
    "\n",
    "# 重命名字典\n",
    "rename_dict = {\n",
    "            'item_id' : 'item_path',\n",
    "            'cat_id' : 'cat_path',\n",
    "            'seller_id' : 'seller_path',\n",
    "            'brand_id' : 'brand_path',\n",
    "            'time_stamp' : 'time_stamp_path',\n",
    "            'action_type' : 'action_type_path'\n",
    "        }\n",
    "\n",
    "# 合并函数\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    # 处理日志文件\n",
    "    df_data = df_data.\\\n",
    "            groupby(join_columns).\\\n",
    "            agg(agg_dict).\\\n",
    "            reset_index().\\\n",
    "            rename(columns=rename_dict)\n",
    "    # 合并全数据与日志文件\n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how=\"left\")\n",
    "    return df_ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n",
       "      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n",
       "      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n",
       "      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n",
       "      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n",
       "      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n",
       "      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n",
       "      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n",
       "      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n",
       "      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>581818 879005 581818 581818 1011673 52343 2773...</td>\n",
       "      <td>1505 662 1505 1505 1505 662 1095 1505 662 1095...</td>\n",
       "      <td>416 3606 416 416 416 3760 3606 416 1926 3004 4...</td>\n",
       "      <td>4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>191923 191923 191923 191923 964906 229470 2294...</td>\n",
       "      <td>1023 1023 1023 1023 662 664 664 1544 664 662 6...</td>\n",
       "      <td>3545 3545 3545 3545 4566 2537 2537 2420 2537 4...</td>\n",
       "      <td>5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...</td>\n",
       "      <td>601 601 601 601 614 614 614 614 614 614 618 61...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  prob  age_range  gender  \\\n",
       "0    34176         3906    0.0   NaN        6.0     0.0   \n",
       "1    34176          121    0.0   NaN        6.0     0.0   \n",
       "2    34176         4356    1.0   NaN        6.0     0.0   \n",
       "3    34176         2217    0.0   NaN        6.0     0.0   \n",
       "4   230784         4818    0.0   NaN        0.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  581818 879005 581818 581818 1011673 52343 2773...   \n",
       "1  581818 879005 581818 581818 1011673 52343 2773...   \n",
       "2  581818 879005 581818 581818 1011673 52343 2773...   \n",
       "3  581818 879005 581818 581818 1011673 52343 2773...   \n",
       "4  191923 191923 191923 191923 964906 229470 2294...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
       "1  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
       "2  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
       "3  1505 662 1505 1505 1505 662 1095 1505 662 1095...   \n",
       "4  1023 1023 1023 1023 662 664 664 1544 664 662 6...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
       "1  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
       "2  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
       "3  416 3606 416 416 416 3760 3606 416 1926 3004 4...   \n",
       "4  3545 3545 3545 3545 4566 2537 2537 2420 2537 4...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
       "1  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
       "2  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
       "3  4014.0 33.0 4014.0 4014.0 4014.0 3738.0 33.0 4...   \n",
       "4  5860.0 5860.0 5860.0 5860.0 6322.0 6066.0 6066...   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "1  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "2  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "3  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "4  601 601 601 601 614 614 614 614 614 614 618 61...   \n",
       "\n",
       "                                    action_type_path  \n",
       "0  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
       "1  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
       "2  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
       "3  0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 2 ...  \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除不需要的数据\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程（copy）\n",
    "**仅为举例，故只截取数据集的前2000跑下列代码**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.基于统计学的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计总数\n",
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# 统计唯一总数\n",
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# 统计最大值\n",
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# 统计最小值\n",
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1   \n",
    "\n",
    "# 统计标准差\n",
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1 \n",
    "    \n",
    "# 获取top(n)的对象\n",
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# 获取top(n)的对象数量\n",
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "    \n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "    \n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "all_data_test = all_data.head(2000)\n",
    "#all_data_test = all_data\n",
    "# 统计用户 点击、浏览、加购、购买行为\n",
    "# 用户在日志上和商户交互的总次数\n",
    "all_data_test = user_cnt(all_data_test,  'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test,  'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test,  'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test,  'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test,  'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test,  'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不同行为种数\n",
    "all_data_test = user_nunique(all_data_test,  'action_type_path', 'action_type_nunique')\n",
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test,  'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test,  'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test,  'action_type_path', 'time_stamp_std')\n",
    "# 最早和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']\n",
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# .....\n",
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 最喜欢的类目 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', n=1)\n",
    "# .....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击、加购、购买、收藏 分开统计\n",
    "\"\"\"\n",
    "统计基本特征函数  \n",
    "-- 知识点二\n",
    "-- 根据不同行为的业务函数\n",
    "-- 提取不同特征\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type is not None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col_list[0]])\n",
    "        \n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in col_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(data_out)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type is not None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col_list[0]])\n",
    "        \n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in col_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis='columns')\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis='columns')\n",
    "    return df_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**组合特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path', 'item_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'prob', 'age_range', 'gender',\n",
       "       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n",
       "       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.利用Countvector和TF-IDF提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点四\n",
    "-- 利用countvector，tfidf提取特征\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "# cntVec = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "\n",
    "\n",
    "# columns_list = ['seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "\ttfidfVec.fit(all_data_test[col])\n",
    "\tdata_ = tfidfVec.transform(all_data_test[col])\n",
    "\tif i == 0:\n",
    "\t\tdata_cat = data_\n",
    "\telse:\n",
    "\t\tdata_cat = sparse.hstack((data_cat, data_))\n",
    "\n",
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.嵌入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')), vector_size=100, window=5, min_count=5, workers=4)\n",
    "# model.save(\"product2vec.model\")\n",
    "# model = gensim.models.Word2Vec.load(\"product2vec.model\")\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i \n",
    "    except:\n",
    "        return  np.zeros(size)\n",
    "\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]\n",
    "\n",
    "all_data_test = pd.concat([all_data_test, df_embeeding],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1stacking 回归特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 回归\n",
    "-- stacking 回归特征\n",
    "\"\"\"\n",
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test,\"lgb_reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2stacking 分类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            \n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=None, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练集/验证集\n",
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values\n",
    "\n",
    "# 处理nan值和inf值\n",
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data\n",
    "\n",
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入划分数据函数 设stacking特征为5折\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# 使用lgb和xgb分类模型构造stacking特征\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\basic.py:1487: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6301\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 122\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.066541\n",
      "[LightGBM] [Info] Start training from score -2.743030\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.240354\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.240074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.239872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.239476\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.239262\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.239001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.238826\n",
      "[8]\tvalid_0's multi_logloss: 0.238887\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.238788\n",
      "[10]\tvalid_0's multi_logloss: 0.23872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.238709\n",
      "[12]\tvalid_0's multi_logloss: 0.23856\n",
      "[13]\tvalid_0's multi_logloss: 0.238573\n",
      "[14]\tvalid_0's multi_logloss: 0.238704\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's multi_logloss: 0.238596\n",
      "[16]\tvalid_0's multi_logloss: 0.238686\n",
      "[17]\tvalid_0's multi_logloss: 0.238653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.238633\n",
      "[19]\tvalid_0's multi_logloss: 0.238698\n",
      "[20]\tvalid_0's multi_logloss: 0.238663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's multi_logloss: 0.238696\n",
      "[22]\tvalid_0's multi_logloss: 0.238428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.238578\n",
      "[24]\tvalid_0's multi_logloss: 0.238486\n",
      "[25]\tvalid_0's multi_logloss: 0.238631\n",
      "[26]\tvalid_0's multi_logloss: 0.23872\n",
      "[27]\tvalid_0's multi_logloss: 0.238687\n",
      "[28]\tvalid_0's multi_logloss: 0.238763\n",
      "[29]\tvalid_0's multi_logloss: 0.238617\n",
      "[30]\tvalid_0's multi_logloss: 0.238621\n",
      "[31]\tvalid_0's multi_logloss: 0.238649\n",
      "[32]\tvalid_0's multi_logloss: 0.238812\n",
      "[33]\tvalid_0's multi_logloss: 0.238682\n",
      "[34]\tvalid_0's multi_logloss: 0.238879\n",
      "[35]\tvalid_0's multi_logloss: 0.238828\n",
      "[36]\tvalid_0's multi_logloss: 0.238651\n",
      "[37]\tvalid_0's multi_logloss: 0.238714\n",
      "[38]\tvalid_0's multi_logloss: 0.238588\n",
      "[39]\tvalid_0's multi_logloss: 0.238711\n",
      "[40]\tvalid_0's multi_logloss: 0.238835\n",
      "[41]\tvalid_0's multi_logloss: 0.238907\n",
      "[42]\tvalid_0's multi_logloss: 0.23879\n",
      "[43]\tvalid_0's multi_logloss: 0.238723\n",
      "[44]\tvalid_0's multi_logloss: 0.238999\n",
      "[45]\tvalid_0's multi_logloss: 0.2392\n",
      "[46]\tvalid_0's multi_logloss: 0.239266\n",
      "[47]\tvalid_0's multi_logloss: 0.23938\n",
      "[48]\tvalid_0's multi_logloss: 0.23936\n",
      "[49]\tvalid_0's multi_logloss: 0.239577\n",
      "[50]\tvalid_0's multi_logloss: 0.239428\n",
      "[51]\tvalid_0's multi_logloss: 0.239767\n",
      "[52]\tvalid_0's multi_logloss: 0.239776\n",
      "[53]\tvalid_0's multi_logloss: 0.239871\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.240046\n",
      "[55]\tvalid_0's multi_logloss: 0.240217\n",
      "[56]\tvalid_0's multi_logloss: 0.240306\n",
      "[57]\tvalid_0's multi_logloss: 0.240222\n",
      "[58]\tvalid_0's multi_logloss: 0.240461\n",
      "[59]\tvalid_0's multi_logloss: 0.240595\n",
      "[60]\tvalid_0's multi_logloss: 0.240455\n",
      "[61]\tvalid_0's multi_logloss: 0.24068\n",
      "[62]\tvalid_0's multi_logloss: 0.240774\n",
      "[63]\tvalid_0's multi_logloss: 0.240656\n",
      "[64]\tvalid_0's multi_logloss: 0.240826\n",
      "[65]\tvalid_0's multi_logloss: 0.240745\n",
      "[66]\tvalid_0's multi_logloss: 0.24084\n",
      "[67]\tvalid_0's multi_logloss: 0.240737\n",
      "[68]\tvalid_0's multi_logloss: 0.240741\n",
      "[69]\tvalid_0's multi_logloss: 0.240675\n",
      "[70]\tvalid_0's multi_logloss: 0.240695\n",
      "[71]\tvalid_0's multi_logloss: 0.240908\n",
      "[72]\tvalid_0's multi_logloss: 0.241034\n",
      "[73]\tvalid_0's multi_logloss: 0.241195\n",
      "[74]\tvalid_0's multi_logloss: 0.241319\n",
      "[75]\tvalid_0's multi_logloss: 0.241434\n",
      "[76]\tvalid_0's multi_logloss: 0.241436\n",
      "[77]\tvalid_0's multi_logloss: 0.241719\n",
      "[78]\tvalid_0's multi_logloss: 0.241881\n",
      "[79]\tvalid_0's multi_logloss: 0.241876\n",
      "[80]\tvalid_0's multi_logloss: 0.241924\n",
      "[81]\tvalid_0's multi_logloss: 0.24207\n",
      "[82]\tvalid_0's multi_logloss: 0.242254\n",
      "[83]\tvalid_0's multi_logloss: 0.242372\n",
      "[84]\tvalid_0's multi_logloss: 0.242374\n",
      "[85]\tvalid_0's multi_logloss: 0.242678\n",
      "[86]\tvalid_0's multi_logloss: 0.2429\n",
      "[87]\tvalid_0's multi_logloss: 0.242889\n",
      "[88]\tvalid_0's multi_logloss: 0.242938\n",
      "[89]\tvalid_0's multi_logloss: 0.243\n",
      "[90]\tvalid_0's multi_logloss: 0.243134\n",
      "[91]\tvalid_0's multi_logloss: 0.243174\n",
      "[92]\tvalid_0's multi_logloss: 0.243298\n",
      "[93]\tvalid_0's multi_logloss: 0.243278\n",
      "[94]\tvalid_0's multi_logloss: 0.243459\n",
      "[95]\tvalid_0's multi_logloss: 0.243589\n",
      "[96]\tvalid_0's multi_logloss: 0.243766\n",
      "[97]\tvalid_0's multi_logloss: 0.243819\n",
      "[98]\tvalid_0's multi_logloss: 0.243849\n",
      "[99]\tvalid_0's multi_logloss: 0.243893\n",
      "[100]\tvalid_0's multi_logloss: 0.244063\n",
      "[101]\tvalid_0's multi_logloss: 0.244275\n",
      "[102]\tvalid_0's multi_logloss: 0.244283\n",
      "[103]\tvalid_0's multi_logloss: 0.244363\n",
      "[104]\tvalid_0's multi_logloss: 0.244471\n",
      "[105]\tvalid_0's multi_logloss: 0.244603\n",
      "[106]\tvalid_0's multi_logloss: 0.244707\n",
      "[107]\tvalid_0's multi_logloss: 0.244949\n",
      "[108]\tvalid_0's multi_logloss: 0.245038\n",
      "[109]\tvalid_0's multi_logloss: 0.245082\n",
      "[110]\tvalid_0's multi_logloss: 0.24532\n",
      "[111]\tvalid_0's multi_logloss: 0.245316\n",
      "[112]\tvalid_0's multi_logloss: 0.245451\n",
      "[113]\tvalid_0's multi_logloss: 0.245559\n",
      "[114]\tvalid_0's multi_logloss: 0.245689\n",
      "[115]\tvalid_0's multi_logloss: 0.245794\n",
      "[116]\tvalid_0's multi_logloss: 0.245792\n",
      "[117]\tvalid_0's multi_logloss: 0.246017\n",
      "[118]\tvalid_0's multi_logloss: 0.246106\n",
      "[119]\tvalid_0's multi_logloss: 0.246056\n",
      "[120]\tvalid_0's multi_logloss: 0.246087\n",
      "[121]\tvalid_0's multi_logloss: 0.246332\n",
      "[122]\tvalid_0's multi_logloss: 0.246362\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid_0's multi_logloss: 0.238428\n",
      "lgb now score is: [2.6379437533193766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\basic.py:1487: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6393\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 123\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.062541\n",
      "[LightGBM] [Info] Start training from score -2.803048\n",
      "[1]\tvalid_0's multi_logloss: 0.28194\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.281973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.282284\n",
      "[4]\tvalid_0's multi_logloss: 0.282817\n",
      "[5]\tvalid_0's multi_logloss: 0.282826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.283212\n",
      "[7]\tvalid_0's multi_logloss: 0.283308\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.283591\n",
      "[9]\tvalid_0's multi_logloss: 0.283664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.284029\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.284255\n",
      "[12]\tvalid_0's multi_logloss: 0.284326\n",
      "[13]\tvalid_0's multi_logloss: 0.284267\n",
      "[14]\tvalid_0's multi_logloss: 0.284805\n",
      "[15]\tvalid_0's multi_logloss: 0.284884\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 0.285103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's multi_logloss: 0.285356\n",
      "[18]\tvalid_0's multi_logloss: 0.285638\n",
      "[19]\tvalid_0's multi_logloss: 0.285689\n",
      "[20]\tvalid_0's multi_logloss: 0.285838\n",
      "[21]\tvalid_0's multi_logloss: 0.286015\n",
      "[22]\tvalid_0's multi_logloss: 0.286296\n",
      "[23]\tvalid_0's multi_logloss: 0.286337\n",
      "[24]\tvalid_0's multi_logloss: 0.286576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's multi_logloss: 0.286995\n",
      "[26]\tvalid_0's multi_logloss: 0.287263\n",
      "[27]\tvalid_0's multi_logloss: 0.28738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.287654\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's multi_logloss: 0.287976\n",
      "[30]\tvalid_0's multi_logloss: 0.28817\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's multi_logloss: 0.288517\n",
      "[32]\tvalid_0's multi_logloss: 0.288985\n",
      "[33]\tvalid_0's multi_logloss: 0.289088\n",
      "[34]\tvalid_0's multi_logloss: 0.289342\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.289747\n",
      "[36]\tvalid_0's multi_logloss: 0.290322\n",
      "[37]\tvalid_0's multi_logloss: 0.290649\n",
      "[38]\tvalid_0's multi_logloss: 0.291138\n",
      "[39]\tvalid_0's multi_logloss: 0.29149\n",
      "[40]\tvalid_0's multi_logloss: 0.291643\n",
      "[41]\tvalid_0's multi_logloss: 0.291891\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's multi_logloss: 0.292378\n",
      "[43]\tvalid_0's multi_logloss: 0.292793\n",
      "[44]\tvalid_0's multi_logloss: 0.293204\n",
      "[45]\tvalid_0's multi_logloss: 0.293478\n",
      "[46]\tvalid_0's multi_logloss: 0.293966\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.294358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.294606\n",
      "[49]\tvalid_0's multi_logloss: 0.294884\n",
      "[50]\tvalid_0's multi_logloss: 0.295165\n",
      "[51]\tvalid_0's multi_logloss: 0.29584\n",
      "[52]\tvalid_0's multi_logloss: 0.296014\n",
      "[53]\tvalid_0's multi_logloss: 0.296442\n",
      "[54]\tvalid_0's multi_logloss: 0.296979\n",
      "[55]\tvalid_0's multi_logloss: 0.297159\n",
      "[56]\tvalid_0's multi_logloss: 0.297605\n",
      "[57]\tvalid_0's multi_logloss: 0.29789\n",
      "[58]\tvalid_0's multi_logloss: 0.298223\n",
      "[59]\tvalid_0's multi_logloss: 0.298525\n",
      "[60]\tvalid_0's multi_logloss: 0.298734\n",
      "[61]\tvalid_0's multi_logloss: 0.299102\n",
      "[62]\tvalid_0's multi_logloss: 0.299221\n",
      "[63]\tvalid_0's multi_logloss: 0.299277\n",
      "[64]\tvalid_0's multi_logloss: 0.299576\n",
      "[65]\tvalid_0's multi_logloss: 0.299937\n",
      "[66]\tvalid_0's multi_logloss: 0.300184\n",
      "[67]\tvalid_0's multi_logloss: 0.300624\n",
      "[68]\tvalid_0's multi_logloss: 0.301036\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's multi_logloss: 0.30131\n",
      "[70]\tvalid_0's multi_logloss: 0.301585\n",
      "[71]\tvalid_0's multi_logloss: 0.302218\n",
      "[72]\tvalid_0's multi_logloss: 0.302574\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's multi_logloss: 0.302811\n",
      "[74]\tvalid_0's multi_logloss: 0.303176\n",
      "[75]\tvalid_0's multi_logloss: 0.303486\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's multi_logloss: 0.303985\n",
      "[77]\tvalid_0's multi_logloss: 0.304359\n",
      "[78]\tvalid_0's multi_logloss: 0.304763\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's multi_logloss: 0.305179\n",
      "[80]\tvalid_0's multi_logloss: 0.305436\n",
      "[81]\tvalid_0's multi_logloss: 0.305726\n",
      "[82]\tvalid_0's multi_logloss: 0.305944\n",
      "[83]\tvalid_0's multi_logloss: 0.305825\n",
      "[84]\tvalid_0's multi_logloss: 0.306226\n",
      "[85]\tvalid_0's multi_logloss: 0.306477\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.306839\n",
      "[87]\tvalid_0's multi_logloss: 0.307239\n",
      "[88]\tvalid_0's multi_logloss: 0.307581\n",
      "[89]\tvalid_0's multi_logloss: 0.307921\n",
      "[90]\tvalid_0's multi_logloss: 0.30839\n",
      "[91]\tvalid_0's multi_logloss: 0.308826\n",
      "[92]\tvalid_0's multi_logloss: 0.309098\n",
      "[93]\tvalid_0's multi_logloss: 0.309558\n",
      "[94]\tvalid_0's multi_logloss: 0.309802\n",
      "[95]\tvalid_0's multi_logloss: 0.31016\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[96]\tvalid_0's multi_logloss: 0.310299\n",
      "[97]\tvalid_0's multi_logloss: 0.310492\n",
      "[98]\tvalid_0's multi_logloss: 0.310939\n",
      "[99]\tvalid_0's multi_logloss: 0.311202\n",
      "[100]\tvalid_0's multi_logloss: 0.311411\n",
      "[101]\tvalid_0's multi_logloss: 0.312016\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.28194\n",
      "lgb now score is: [2.6379437533193766, 2.588247072112946]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6368\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 122\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065205\n",
      "[LightGBM] [Info] Start training from score -2.762638\n",
      "[1]\tvalid_0's multi_logloss: 0.254254\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.254523\n",
      "[3]\tvalid_0's multi_logloss: 0.254773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.255089\n",
      "[5]\tvalid_0's multi_logloss: 0.255165\n",
      "[6]\tvalid_0's multi_logloss: 0.255359\n",
      "[7]\tvalid_0's multi_logloss: 0.255786\n",
      "[8]\tvalid_0's multi_logloss: 0.256196\n",
      "[9]\tvalid_0's multi_logloss: 0.256413\n",
      "[10]\tvalid_0's multi_logloss: 0.256875\n",
      "[11]\tvalid_0's multi_logloss: 0.257176\n",
      "[12]\tvalid_0's multi_logloss: 0.257695\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.258109\n",
      "[14]\tvalid_0's multi_logloss: 0.258423\n",
      "[15]\tvalid_0's multi_logloss: 0.258742\n",
      "[16]\tvalid_0's multi_logloss: 0.259123\n",
      "[17]\tvalid_0's multi_logloss: 0.259672\n",
      "[18]\tvalid_0's multi_logloss: 0.259966\n",
      "[19]\tvalid_0's multi_logloss: 0.26036\n",
      "[20]\tvalid_0's multi_logloss: 0.260518\n",
      "[21]\tvalid_0's multi_logloss: 0.260726\n",
      "[22]\tvalid_0's multi_logloss: 0.261058\n",
      "[23]\tvalid_0's multi_logloss: 0.261381\n",
      "[24]\tvalid_0's multi_logloss: 0.261417\n",
      "[25]\tvalid_0's multi_logloss: 0.261486\n",
      "[26]\tvalid_0's multi_logloss: 0.261682\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.26194\n",
      "[28]\tvalid_0's multi_logloss: 0.262079\n",
      "[29]\tvalid_0's multi_logloss: 0.262502\n",
      "[30]\tvalid_0's multi_logloss: 0.26289\n",
      "[31]\tvalid_0's multi_logloss: 0.263311\n",
      "[32]\tvalid_0's multi_logloss: 0.263552\n",
      "[33]\tvalid_0's multi_logloss: 0.263709\n",
      "[34]\tvalid_0's multi_logloss: 0.26424\n",
      "[35]\tvalid_0's multi_logloss: 0.264636\n",
      "[36]\tvalid_0's multi_logloss: 0.264834\n",
      "[37]\tvalid_0's multi_logloss: 0.265138\n",
      "[38]\tvalid_0's multi_logloss: 0.265541\n",
      "[39]\tvalid_0's multi_logloss: 0.265681\n",
      "[40]\tvalid_0's multi_logloss: 0.265898\n",
      "[41]\tvalid_0's multi_logloss: 0.266109\n",
      "[42]\tvalid_0's multi_logloss: 0.266542\n",
      "[43]\tvalid_0's multi_logloss: 0.266803\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's multi_logloss: 0.267107\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.267373\n",
      "[46]\tvalid_0's multi_logloss: 0.267543\n",
      "[47]\tvalid_0's multi_logloss: 0.267821\n",
      "[48]\tvalid_0's multi_logloss: 0.267974\n",
      "[49]\tvalid_0's multi_logloss: 0.268061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's multi_logloss: 0.268349\n",
      "[51]\tvalid_0's multi_logloss: 0.268618\n",
      "[52]\tvalid_0's multi_logloss: 0.269029\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's multi_logloss: 0.269526\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.269913\n",
      "[55]\tvalid_0's multi_logloss: 0.269961\n",
      "[56]\tvalid_0's multi_logloss: 0.270079\n",
      "[57]\tvalid_0's multi_logloss: 0.270351\n",
      "[58]\tvalid_0's multi_logloss: 0.27055\n",
      "[59]\tvalid_0's multi_logloss: 0.27087\n",
      "[60]\tvalid_0's multi_logloss: 0.271432\n",
      "[61]\tvalid_0's multi_logloss: 0.271915\n",
      "[62]\tvalid_0's multi_logloss: 0.272188\n",
      "[63]\tvalid_0's multi_logloss: 0.272697\n",
      "[64]\tvalid_0's multi_logloss: 0.272742\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's multi_logloss: 0.272959\n",
      "[66]\tvalid_0's multi_logloss: 0.27326\n",
      "[67]\tvalid_0's multi_logloss: 0.273381\n",
      "[68]\tvalid_0's multi_logloss: 0.273526\n",
      "[69]\tvalid_0's multi_logloss: 0.273775\n",
      "[70]\tvalid_0's multi_logloss: 0.273897\n",
      "[71]\tvalid_0's multi_logloss: 0.274165\n",
      "[72]\tvalid_0's multi_logloss: 0.27428\n",
      "[73]\tvalid_0's multi_logloss: 0.274675\n",
      "[74]\tvalid_0's multi_logloss: 0.275069\n",
      "[75]\tvalid_0's multi_logloss: 0.275254\n",
      "[76]\tvalid_0's multi_logloss: 0.27555\n",
      "[77]\tvalid_0's multi_logloss: 0.275878\n",
      "[78]\tvalid_0's multi_logloss: 0.276016\n",
      "[79]\tvalid_0's multi_logloss: 0.276544\n",
      "[80]\tvalid_0's multi_logloss: 0.27684\n",
      "[81]\tvalid_0's multi_logloss: 0.277169\n",
      "[82]\tvalid_0's multi_logloss: 0.277296\n",
      "[83]\tvalid_0's multi_logloss: 0.277625\n",
      "[84]\tvalid_0's multi_logloss: 0.277928\n",
      "[85]\tvalid_0's multi_logloss: 0.278246\n",
      "[86]\tvalid_0's multi_logloss: 0.278657\n",
      "[87]\tvalid_0's multi_logloss: 0.278905\n",
      "[88]\tvalid_0's multi_logloss: 0.279373\n",
      "[89]\tvalid_0's multi_logloss: 0.27979\n",
      "[90]\tvalid_0's multi_logloss: 0.280256\n",
      "[91]\tvalid_0's multi_logloss: 0.280584\n",
      "[92]\tvalid_0's multi_logloss: 0.280932\n",
      "[93]\tvalid_0's multi_logloss: 0.281369\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[94]\tvalid_0's multi_logloss: 0.281835\n",
      "[95]\tvalid_0's multi_logloss: 0.282097\n",
      "[96]\tvalid_0's multi_logloss: 0.282481\n",
      "[97]\tvalid_0's multi_logloss: 0.282839\n",
      "[98]\tvalid_0's multi_logloss: 0.283182\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[99]\tvalid_0's multi_logloss: 0.283567\n",
      "[100]\tvalid_0's multi_logloss: 0.283868\n",
      "[101]\tvalid_0's multi_logloss: 0.284006\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.254254\n",
      "lgb now score is: [2.6379437533193766, 2.588247072112946, 2.57817901986284]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\basic.py:1487: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\basic.py:1487: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6304\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 122\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.069886\n",
      "[LightGBM] [Info] Start training from score -2.695628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.207445\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.20716\n",
      "[3]\tvalid_0's multi_logloss: 0.206722\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.206342\n",
      "[5]\tvalid_0's multi_logloss: 0.20616\n",
      "[6]\tvalid_0's multi_logloss: 0.205879\n",
      "[7]\tvalid_0's multi_logloss: 0.205756\n",
      "[8]\tvalid_0's multi_logloss: 0.205224\n",
      "[9]\tvalid_0's multi_logloss: 0.205093\n",
      "[10]\tvalid_0's multi_logloss: 0.204971\n",
      "[11]\tvalid_0's multi_logloss: 0.204802\n",
      "[12]\tvalid_0's multi_logloss: 0.204506\n",
      "[13]\tvalid_0's multi_logloss: 0.204474\n",
      "[14]\tvalid_0's multi_logloss: 0.204266\n",
      "[15]\tvalid_0's multi_logloss: 0.204136\n",
      "[16]\tvalid_0's multi_logloss: 0.203918\n",
      "[17]\tvalid_0's multi_logloss: 0.203726\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.203396\n",
      "[19]\tvalid_0's multi_logloss: 0.20337\n",
      "[20]\tvalid_0's multi_logloss: 0.203262\n",
      "[21]\tvalid_0's multi_logloss: 0.202947\n",
      "[22]\tvalid_0's multi_logloss: 0.20267\n",
      "[23]\tvalid_0's multi_logloss: 0.202468\n",
      "[24]\tvalid_0's multi_logloss: 0.202428\n",
      "[25]\tvalid_0's multi_logloss: 0.202344\n",
      "[26]\tvalid_0's multi_logloss: 0.202124\n",
      "[27]\tvalid_0's multi_logloss: 0.202044\n",
      "[28]\tvalid_0's multi_logloss: 0.201988\n",
      "[29]\tvalid_0's multi_logloss: 0.201839\n",
      "[30]\tvalid_0's multi_logloss: 0.201686\n",
      "[31]\tvalid_0's multi_logloss: 0.201795\n",
      "[32]\tvalid_0's multi_logloss: 0.201775\n",
      "[33]\tvalid_0's multi_logloss: 0.2018\n",
      "[34]\tvalid_0's multi_logloss: 0.202052\n",
      "[35]\tvalid_0's multi_logloss: 0.201969\n",
      "[36]\tvalid_0's multi_logloss: 0.202082\n",
      "[37]\tvalid_0's multi_logloss: 0.201974\n",
      "[38]\tvalid_0's multi_logloss: 0.202026\n",
      "[39]\tvalid_0's multi_logloss: 0.202045\n",
      "[40]\tvalid_0's multi_logloss: 0.201985\n",
      "[41]\tvalid_0's multi_logloss: 0.201968\n",
      "[42]\tvalid_0's multi_logloss: 0.201982\n",
      "[43]\tvalid_0's multi_logloss: 0.20223\n",
      "[44]\tvalid_0's multi_logloss: 0.202271\n",
      "[45]\tvalid_0's multi_logloss: 0.202133\n",
      "[46]\tvalid_0's multi_logloss: 0.202015\n",
      "[47]\tvalid_0's multi_logloss: 0.202118\n",
      "[48]\tvalid_0's multi_logloss: 0.201993\n",
      "[49]\tvalid_0's multi_logloss: 0.202076\n",
      "[50]\tvalid_0's multi_logloss: 0.202213\n",
      "[51]\tvalid_0's multi_logloss: 0.20229\n",
      "[52]\tvalid_0's multi_logloss: 0.202314\n",
      "[53]\tvalid_0's multi_logloss: 0.202442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's multi_logloss: 0.202436\n",
      "[55]\tvalid_0's multi_logloss: 0.202408\n",
      "[56]\tvalid_0's multi_logloss: 0.202413\n",
      "[57]\tvalid_0's multi_logloss: 0.202458\n",
      "[58]\tvalid_0's multi_logloss: 0.202528\n",
      "[59]\tvalid_0's multi_logloss: 0.202706\n",
      "[60]\tvalid_0's multi_logloss: 0.202853\n",
      "[61]\tvalid_0's multi_logloss: 0.202839\n",
      "[62]\tvalid_0's multi_logloss: 0.20277\n",
      "[63]\tvalid_0's multi_logloss: 0.202794\n",
      "[64]\tvalid_0's multi_logloss: 0.202875\n",
      "[65]\tvalid_0's multi_logloss: 0.202946\n",
      "[66]\tvalid_0's multi_logloss: 0.203173\n",
      "[67]\tvalid_0's multi_logloss: 0.203442\n",
      "[68]\tvalid_0's multi_logloss: 0.203538\n",
      "[69]\tvalid_0's multi_logloss: 0.203756\n",
      "[70]\tvalid_0's multi_logloss: 0.203957\n",
      "[71]\tvalid_0's multi_logloss: 0.204248\n",
      "[72]\tvalid_0's multi_logloss: 0.204437\n",
      "[73]\tvalid_0's multi_logloss: 0.204634\n",
      "[74]\tvalid_0's multi_logloss: 0.204744\n",
      "[75]\tvalid_0's multi_logloss: 0.204722\n",
      "[76]\tvalid_0's multi_logloss: 0.204884\n",
      "[77]\tvalid_0's multi_logloss: 0.205224\n",
      "[78]\tvalid_0's multi_logloss: 0.205304\n",
      "[79]\tvalid_0's multi_logloss: 0.205344\n",
      "[80]\tvalid_0's multi_logloss: 0.205535\n",
      "[81]\tvalid_0's multi_logloss: 0.205829\n",
      "[82]\tvalid_0's multi_logloss: 0.205903\n",
      "[83]\tvalid_0's multi_logloss: 0.206216\n",
      "[84]\tvalid_0's multi_logloss: 0.206404\n",
      "[85]\tvalid_0's multi_logloss: 0.206529\n",
      "[86]\tvalid_0's multi_logloss: 0.206723\n",
      "[87]\tvalid_0's multi_logloss: 0.206825\n",
      "[88]\tvalid_0's multi_logloss: 0.206916\n",
      "[89]\tvalid_0's multi_logloss: 0.20718\n",
      "[90]\tvalid_0's multi_logloss: 0.207473\n",
      "[91]\tvalid_0's multi_logloss: 0.207615\n",
      "[92]\tvalid_0's multi_logloss: 0.207871\n",
      "[93]\tvalid_0's multi_logloss: 0.208164\n",
      "[94]\tvalid_0's multi_logloss: 0.208306\n",
      "[95]\tvalid_0's multi_logloss: 0.208548\n",
      "[96]\tvalid_0's multi_logloss: 0.208783\n",
      "[97]\tvalid_0's multi_logloss: 0.208939\n",
      "[98]\tvalid_0's multi_logloss: 0.209008\n",
      "[99]\tvalid_0's multi_logloss: 0.209224\n",
      "[100]\tvalid_0's multi_logloss: 0.209281\n",
      "[101]\tvalid_0's multi_logloss: 0.20928\n",
      "[102]\tvalid_0's multi_logloss: 0.209452\n",
      "[103]\tvalid_0's multi_logloss: 0.209695\n",
      "[104]\tvalid_0's multi_logloss: 0.209923\n",
      "[105]\tvalid_0's multi_logloss: 0.210125\n",
      "[106]\tvalid_0's multi_logloss: 0.210284\n",
      "[107]\tvalid_0's multi_logloss: 0.210535\n",
      "[108]\tvalid_0's multi_logloss: 0.210655\n",
      "[109]\tvalid_0's multi_logloss: 0.210887\n",
      "[110]\tvalid_0's multi_logloss: 0.211044\n",
      "[111]\tvalid_0's multi_logloss: 0.211209\n",
      "[112]\tvalid_0's multi_logloss: 0.211365\n",
      "[113]\tvalid_0's multi_logloss: 0.211628\n",
      "[114]\tvalid_0's multi_logloss: 0.21176\n",
      "[115]\tvalid_0's multi_logloss: 0.211776\n",
      "[116]\tvalid_0's multi_logloss: 0.212017\n",
      "[117]\tvalid_0's multi_logloss: 0.21223\n",
      "[118]\tvalid_0's multi_logloss: 0.212156\n",
      "[119]\tvalid_0's multi_logloss: 0.212514\n",
      "[120]\tvalid_0's multi_logloss: 0.21272\n",
      "[121]\tvalid_0's multi_logloss: 0.212928\n",
      "[122]\tvalid_0's multi_logloss: 0.213081\n",
      "[123]\tvalid_0's multi_logloss: 0.213286\n",
      "[124]\tvalid_0's multi_logloss: 0.213309\n",
      "[125]\tvalid_0's multi_logloss: 0.213505\n",
      "[126]\tvalid_0's multi_logloss: 0.213522\n",
      "[127]\tvalid_0's multi_logloss: 0.213487\n",
      "[128]\tvalid_0's multi_logloss: 0.213572\n",
      "[129]\tvalid_0's multi_logloss: 0.213699\n",
      "[130]\tvalid_0's multi_logloss: 0.213814\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid_0's multi_logloss: 0.201686\n",
      "lgb now score is: [2.6379437533193766, 2.588247072112946, 2.57817901986284, 2.6524694314135764]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6281\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 122\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.069216\n",
      "[LightGBM] [Info] Start training from score -2.704930\n",
      "[1]\tvalid_0's multi_logloss: 0.213928\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.213618\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.213331\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.212846\n",
      "[5]\tvalid_0's multi_logloss: 0.212645\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.212674\n",
      "[7]\tvalid_0's multi_logloss: 0.212336\n",
      "[8]\tvalid_0's multi_logloss: 0.211979\n",
      "[9]\tvalid_0's multi_logloss: 0.211506\n",
      "[10]\tvalid_0's multi_logloss: 0.211444\n",
      "[11]\tvalid_0's multi_logloss: 0.211465\n",
      "[12]\tvalid_0's multi_logloss: 0.211141\n",
      "[13]\tvalid_0's multi_logloss: 0.211058\n",
      "[14]\tvalid_0's multi_logloss: 0.210668\n",
      "[15]\tvalid_0's multi_logloss: 0.210451\n",
      "[16]\tvalid_0's multi_logloss: 0.210301\n",
      "[17]\tvalid_0's multi_logloss: 0.210407\n",
      "[18]\tvalid_0's multi_logloss: 0.210369\n",
      "[19]\tvalid_0's multi_logloss: 0.210368\n",
      "[20]\tvalid_0's multi_logloss: 0.210162\n",
      "[21]\tvalid_0's multi_logloss: 0.210087\n",
      "[22]\tvalid_0's multi_logloss: 0.209948\n",
      "[23]\tvalid_0's multi_logloss: 0.209745\n",
      "[24]\tvalid_0's multi_logloss: 0.209913\n",
      "[25]\tvalid_0's multi_logloss: 0.210125\n",
      "[26]\tvalid_0's multi_logloss: 0.21011\n",
      "[27]\tvalid_0's multi_logloss: 0.210003\n",
      "[28]\tvalid_0's multi_logloss: 0.210009\n",
      "[29]\tvalid_0's multi_logloss: 0.210018\n",
      "[30]\tvalid_0's multi_logloss: 0.209903\n",
      "[31]\tvalid_0's multi_logloss: 0.210005\n",
      "[32]\tvalid_0's multi_logloss: 0.210091\n",
      "[33]\tvalid_0's multi_logloss: 0.209913\n",
      "[34]\tvalid_0's multi_logloss: 0.210057\n",
      "[35]\tvalid_0's multi_logloss: 0.209839\n",
      "[36]\tvalid_0's multi_logloss: 0.209931\n",
      "[37]\tvalid_0's multi_logloss: 0.209846\n",
      "[38]\tvalid_0's multi_logloss: 0.209477\n",
      "[39]\tvalid_0's multi_logloss: 0.209505\n",
      "[40]\tvalid_0's multi_logloss: 0.209457\n",
      "[41]\tvalid_0's multi_logloss: 0.20951\n",
      "[42]\tvalid_0's multi_logloss: 0.20935\n",
      "[43]\tvalid_0's multi_logloss: 0.20944\n",
      "[44]\tvalid_0's multi_logloss: 0.209514\n",
      "[45]\tvalid_0's multi_logloss: 0.209667\n",
      "[46]\tvalid_0's multi_logloss: 0.209659\n",
      "[47]\tvalid_0's multi_logloss: 0.20951\n",
      "[48]\tvalid_0's multi_logloss: 0.209468\n",
      "[49]\tvalid_0's multi_logloss: 0.209475\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's multi_logloss: 0.209578\n",
      "[51]\tvalid_0's multi_logloss: 0.209553\n",
      "[52]\tvalid_0's multi_logloss: 0.209778\n",
      "[53]\tvalid_0's multi_logloss: 0.209788\n",
      "[54]\tvalid_0's multi_logloss: 0.209561\n",
      "[55]\tvalid_0's multi_logloss: 0.209576\n",
      "[56]\tvalid_0's multi_logloss: 0.209707\n",
      "[57]\tvalid_0's multi_logloss: 0.209696\n",
      "[58]\tvalid_0's multi_logloss: 0.209662\n",
      "[59]\tvalid_0's multi_logloss: 0.209743\n",
      "[60]\tvalid_0's multi_logloss: 0.209951\n",
      "[61]\tvalid_0's multi_logloss: 0.209975\n",
      "[62]\tvalid_0's multi_logloss: 0.209987\n",
      "[63]\tvalid_0's multi_logloss: 0.210003\n",
      "[64]\tvalid_0's multi_logloss: 0.210132\n",
      "[65]\tvalid_0's multi_logloss: 0.210117\n",
      "[66]\tvalid_0's multi_logloss: 0.210226\n",
      "[67]\tvalid_0's multi_logloss: 0.21006\n",
      "[68]\tvalid_0's multi_logloss: 0.210248\n",
      "[69]\tvalid_0's multi_logloss: 0.210309\n",
      "[70]\tvalid_0's multi_logloss: 0.210334\n",
      "[71]\tvalid_0's multi_logloss: 0.210621\n",
      "[72]\tvalid_0's multi_logloss: 0.210646\n",
      "[73]\tvalid_0's multi_logloss: 0.210761\n",
      "[74]\tvalid_0's multi_logloss: 0.210854\n",
      "[75]\tvalid_0's multi_logloss: 0.210994\n",
      "[76]\tvalid_0's multi_logloss: 0.211172\n",
      "[77]\tvalid_0's multi_logloss: 0.211174\n",
      "[78]\tvalid_0's multi_logloss: 0.211405\n",
      "[79]\tvalid_0's multi_logloss: 0.211484\n",
      "[80]\tvalid_0's multi_logloss: 0.211513\n",
      "[81]\tvalid_0's multi_logloss: 0.211584\n",
      "[82]\tvalid_0's multi_logloss: 0.211651\n",
      "[83]\tvalid_0's multi_logloss: 0.21172\n",
      "[84]\tvalid_0's multi_logloss: 0.211793\n",
      "[85]\tvalid_0's multi_logloss: 0.211776\n",
      "[86]\tvalid_0's multi_logloss: 0.211871\n",
      "[87]\tvalid_0's multi_logloss: 0.212021\n",
      "[88]\tvalid_0's multi_logloss: 0.212175\n",
      "[89]\tvalid_0's multi_logloss: 0.212295\n",
      "[90]\tvalid_0's multi_logloss: 0.212497\n",
      "[91]\tvalid_0's multi_logloss: 0.2124\n",
      "[92]\tvalid_0's multi_logloss: 0.212494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\lightgbm\\basic.py:1487: UserWarning: silent keyword has been found in `params` and will be ignored.\n",
      "Please use silent argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93]\tvalid_0's multi_logloss: 0.212569\n",
      "[94]\tvalid_0's multi_logloss: 0.212714\n",
      "[95]\tvalid_0's multi_logloss: 0.21291\n",
      "[96]\tvalid_0's multi_logloss: 0.213015\n",
      "[97]\tvalid_0's multi_logloss: 0.212903\n",
      "[98]\tvalid_0's multi_logloss: 0.213043\n",
      "[99]\tvalid_0's multi_logloss: 0.213166\n",
      "[100]\tvalid_0's multi_logloss: 0.213229\n",
      "[101]\tvalid_0's multi_logloss: 0.213253\n",
      "[102]\tvalid_0's multi_logloss: 0.213434\n",
      "[103]\tvalid_0's multi_logloss: 0.213585\n",
      "[104]\tvalid_0's multi_logloss: 0.213686\n",
      "[105]\tvalid_0's multi_logloss: 0.213828\n",
      "[106]\tvalid_0's multi_logloss: 0.213972\n",
      "[107]\tvalid_0's multi_logloss: 0.214155\n",
      "[108]\tvalid_0's multi_logloss: 0.214301\n",
      "[109]\tvalid_0's multi_logloss: 0.21451\n",
      "[110]\tvalid_0's multi_logloss: 0.21484\n",
      "[111]\tvalid_0's multi_logloss: 0.215186\n",
      "[112]\tvalid_0's multi_logloss: 0.215316\n",
      "[113]\tvalid_0's multi_logloss: 0.215461\n",
      "[114]\tvalid_0's multi_logloss: 0.215532\n",
      "[115]\tvalid_0's multi_logloss: 0.215575\n",
      "[116]\tvalid_0's multi_logloss: 0.215679\n",
      "[117]\tvalid_0's multi_logloss: 0.215827\n",
      "[118]\tvalid_0's multi_logloss: 0.215829\n",
      "[119]\tvalid_0's multi_logloss: 0.215801\n",
      "[120]\tvalid_0's multi_logloss: 0.215938\n",
      "[121]\tvalid_0's multi_logloss: 0.216255\n",
      "[122]\tvalid_0's multi_logloss: 0.216355\n",
      "[123]\tvalid_0's multi_logloss: 0.21639\n",
      "[124]\tvalid_0's multi_logloss: 0.216777\n",
      "[125]\tvalid_0's multi_logloss: 0.216825\n",
      "[126]\tvalid_0's multi_logloss: 0.217016\n",
      "[127]\tvalid_0's multi_logloss: 0.217177\n",
      "[128]\tvalid_0's multi_logloss: 0.217247\n",
      "[129]\tvalid_0's multi_logloss: 0.217338\n",
      "[130]\tvalid_0's multi_logloss: 0.217419\n",
      "[131]\tvalid_0's multi_logloss: 0.217507\n",
      "[132]\tvalid_0's multi_logloss: 0.217632\n",
      "[133]\tvalid_0's multi_logloss: 0.217824\n",
      "[134]\tvalid_0's multi_logloss: 0.217905\n",
      "[135]\tvalid_0's multi_logloss: 0.218102\n",
      "[136]\tvalid_0's multi_logloss: 0.218356\n",
      "[137]\tvalid_0's multi_logloss: 0.218655\n",
      "[138]\tvalid_0's multi_logloss: 0.218654\n",
      "[139]\tvalid_0's multi_logloss: 0.218836\n",
      "[140]\tvalid_0's multi_logloss: 0.218992\n",
      "[141]\tvalid_0's multi_logloss: 0.219236\n",
      "[142]\tvalid_0's multi_logloss: 0.219317\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's multi_logloss: 0.20935\n",
      "lgb now score is: [2.6379437533193766, 2.588247072112946, 2.57817901986284, 2.6524694314135764, 2.7239116421673613]\n",
      "lgb_score_list: [2.6379437533193766, 2.588247072112946, 2.57817901986284, 2.6524694314135764, 2.7239116421673613]\n",
      "lgb_score_mean: 2.63615018377522\n",
      "[0]\ttrain-mlogloss:0.67092\teval-mlogloss:0.67101\n",
      "[1]\ttrain-mlogloss:0.65009\teval-mlogloss:0.65047\n",
      "[2]\ttrain-mlogloss:0.63051\teval-mlogloss:0.63136\n",
      "[3]\ttrain-mlogloss:0.61200\teval-mlogloss:0.61282\n",
      "[4]\ttrain-mlogloss:0.59438\teval-mlogloss:0.59526\n",
      "[5]\ttrain-mlogloss:0.57765\teval-mlogloss:0.57864\n",
      "[6]\ttrain-mlogloss:0.56189\teval-mlogloss:0.56320\n",
      "[7]\ttrain-mlogloss:0.54668\teval-mlogloss:0.54829\n",
      "[8]\ttrain-mlogloss:0.53251\teval-mlogloss:0.53450\n",
      "[9]\ttrain-mlogloss:0.51867\teval-mlogloss:0.52102\n",
      "[10]\ttrain-mlogloss:0.50573\teval-mlogloss:0.50826\n",
      "[11]\ttrain-mlogloss:0.49337\teval-mlogloss:0.49628\n",
      "[12]\ttrain-mlogloss:0.48173\teval-mlogloss:0.48465\n",
      "[13]\ttrain-mlogloss:0.47057\teval-mlogloss:0.47406\n",
      "[14]\ttrain-mlogloss:0.45983\teval-mlogloss:0.46338\n",
      "[15]\ttrain-mlogloss:0.44994\teval-mlogloss:0.45377\n",
      "[16]\ttrain-mlogloss:0.44038\teval-mlogloss:0.44429\n",
      "[17]\ttrain-mlogloss:0.43114\teval-mlogloss:0.43525\n",
      "[18]\ttrain-mlogloss:0.42233\teval-mlogloss:0.42675\n",
      "[19]\ttrain-mlogloss:0.41376\teval-mlogloss:0.41832\n",
      "[20]\ttrain-mlogloss:0.40555\teval-mlogloss:0.41021\n",
      "[21]\ttrain-mlogloss:0.39799\teval-mlogloss:0.40272\n",
      "[22]\ttrain-mlogloss:0.39046\teval-mlogloss:0.39541\n",
      "[23]\ttrain-mlogloss:0.38345\teval-mlogloss:0.38852\n",
      "[24]\ttrain-mlogloss:0.37670\teval-mlogloss:0.38194\n",
      "[25]\ttrain-mlogloss:0.37030\teval-mlogloss:0.37595\n",
      "[26]\ttrain-mlogloss:0.36399\teval-mlogloss:0.37004\n",
      "[27]\ttrain-mlogloss:0.35809\teval-mlogloss:0.36437\n",
      "[28]\ttrain-mlogloss:0.35236\teval-mlogloss:0.35892\n",
      "[29]\ttrain-mlogloss:0.34686\teval-mlogloss:0.35362\n",
      "[30]\ttrain-mlogloss:0.34160\teval-mlogloss:0.34844\n",
      "[31]\ttrain-mlogloss:0.33662\teval-mlogloss:0.34362\n",
      "[32]\ttrain-mlogloss:0.33181\teval-mlogloss:0.33915\n",
      "[33]\ttrain-mlogloss:0.32724\teval-mlogloss:0.33474\n",
      "[34]\ttrain-mlogloss:0.32270\teval-mlogloss:0.33053\n",
      "[35]\ttrain-mlogloss:0.31845\teval-mlogloss:0.32658\n",
      "[36]\ttrain-mlogloss:0.31435\teval-mlogloss:0.32278\n",
      "[37]\ttrain-mlogloss:0.31042\teval-mlogloss:0.31920\n",
      "[38]\ttrain-mlogloss:0.30663\teval-mlogloss:0.31553\n",
      "[39]\ttrain-mlogloss:0.30302\teval-mlogloss:0.31219\n",
      "[40]\ttrain-mlogloss:0.29944\teval-mlogloss:0.30865\n",
      "[41]\ttrain-mlogloss:0.29620\teval-mlogloss:0.30555\n",
      "[42]\ttrain-mlogloss:0.29292\teval-mlogloss:0.30267\n",
      "[43]\ttrain-mlogloss:0.28974\teval-mlogloss:0.29988\n",
      "[44]\ttrain-mlogloss:0.28676\teval-mlogloss:0.29719\n",
      "[45]\ttrain-mlogloss:0.28363\teval-mlogloss:0.29440\n",
      "[46]\ttrain-mlogloss:0.28088\teval-mlogloss:0.29205\n",
      "[47]\ttrain-mlogloss:0.27836\teval-mlogloss:0.28979\n",
      "[48]\ttrain-mlogloss:0.27583\teval-mlogloss:0.28745\n",
      "[49]\ttrain-mlogloss:0.27345\teval-mlogloss:0.28519\n",
      "[50]\ttrain-mlogloss:0.27098\teval-mlogloss:0.28303\n",
      "[51]\ttrain-mlogloss:0.26880\teval-mlogloss:0.28110\n",
      "[52]\ttrain-mlogloss:0.26673\teval-mlogloss:0.27932\n",
      "[53]\ttrain-mlogloss:0.26447\teval-mlogloss:0.27731\n",
      "[54]\ttrain-mlogloss:0.26251\teval-mlogloss:0.27546\n",
      "[55]\ttrain-mlogloss:0.26069\teval-mlogloss:0.27384\n",
      "[56]\ttrain-mlogloss:0.25881\teval-mlogloss:0.27211\n",
      "[57]\ttrain-mlogloss:0.25707\teval-mlogloss:0.27057\n",
      "[58]\ttrain-mlogloss:0.25535\teval-mlogloss:0.26900\n",
      "[59]\ttrain-mlogloss:0.25367\teval-mlogloss:0.26752\n",
      "[60]\ttrain-mlogloss:0.25194\teval-mlogloss:0.26601\n",
      "[61]\ttrain-mlogloss:0.25038\teval-mlogloss:0.26478\n",
      "[62]\ttrain-mlogloss:0.24893\teval-mlogloss:0.26348\n",
      "[63]\ttrain-mlogloss:0.24753\teval-mlogloss:0.26229\n",
      "[64]\ttrain-mlogloss:0.24615\teval-mlogloss:0.26115\n",
      "[65]\ttrain-mlogloss:0.24468\teval-mlogloss:0.25992\n",
      "[66]\ttrain-mlogloss:0.24327\teval-mlogloss:0.25879\n",
      "[67]\ttrain-mlogloss:0.24203\teval-mlogloss:0.25779\n",
      "[68]\ttrain-mlogloss:0.24078\teval-mlogloss:0.25703\n",
      "[69]\ttrain-mlogloss:0.23943\teval-mlogloss:0.25611\n",
      "[70]\ttrain-mlogloss:0.23838\teval-mlogloss:0.25517\n",
      "[71]\ttrain-mlogloss:0.23737\teval-mlogloss:0.25435\n",
      "[72]\ttrain-mlogloss:0.23622\teval-mlogloss:0.25379\n",
      "[73]\ttrain-mlogloss:0.23513\teval-mlogloss:0.25294\n",
      "[74]\ttrain-mlogloss:0.23419\teval-mlogloss:0.25223\n",
      "[75]\ttrain-mlogloss:0.23297\teval-mlogloss:0.25145\n",
      "[76]\ttrain-mlogloss:0.23187\teval-mlogloss:0.25087\n",
      "[77]\ttrain-mlogloss:0.23088\teval-mlogloss:0.25022\n",
      "[78]\ttrain-mlogloss:0.23004\teval-mlogloss:0.24956\n",
      "[79]\ttrain-mlogloss:0.22920\teval-mlogloss:0.24896\n",
      "[80]\ttrain-mlogloss:0.22836\teval-mlogloss:0.24841\n",
      "[81]\ttrain-mlogloss:0.22756\teval-mlogloss:0.24777\n",
      "[82]\ttrain-mlogloss:0.22676\teval-mlogloss:0.24734\n",
      "[83]\ttrain-mlogloss:0.22581\teval-mlogloss:0.24691\n",
      "[84]\ttrain-mlogloss:0.22505\teval-mlogloss:0.24649\n",
      "[85]\ttrain-mlogloss:0.22436\teval-mlogloss:0.24609\n",
      "[86]\ttrain-mlogloss:0.22363\teval-mlogloss:0.24576\n",
      "[87]\ttrain-mlogloss:0.22305\teval-mlogloss:0.24542\n",
      "[88]\ttrain-mlogloss:0.22229\teval-mlogloss:0.24508\n",
      "[89]\ttrain-mlogloss:0.22160\teval-mlogloss:0.24480\n",
      "[90]\ttrain-mlogloss:0.22079\teval-mlogloss:0.24442\n",
      "[91]\ttrain-mlogloss:0.22001\teval-mlogloss:0.24415\n",
      "[92]\ttrain-mlogloss:0.21923\teval-mlogloss:0.24378\n",
      "[93]\ttrain-mlogloss:0.21867\teval-mlogloss:0.24339\n",
      "[94]\ttrain-mlogloss:0.21798\teval-mlogloss:0.24323\n",
      "[95]\ttrain-mlogloss:0.21732\teval-mlogloss:0.24302\n",
      "[96]\ttrain-mlogloss:0.21672\teval-mlogloss:0.24280\n",
      "[97]\ttrain-mlogloss:0.21591\teval-mlogloss:0.24250\n",
      "[98]\ttrain-mlogloss:0.21533\teval-mlogloss:0.24223\n",
      "[99]\ttrain-mlogloss:0.21460\teval-mlogloss:0.24204\n",
      "[100]\ttrain-mlogloss:0.21398\teval-mlogloss:0.24198\n",
      "[101]\ttrain-mlogloss:0.21341\teval-mlogloss:0.24181\n",
      "[102]\ttrain-mlogloss:0.21278\teval-mlogloss:0.24168\n",
      "[103]\ttrain-mlogloss:0.21210\teval-mlogloss:0.24157\n",
      "[104]\ttrain-mlogloss:0.21167\teval-mlogloss:0.24146\n",
      "[105]\ttrain-mlogloss:0.21102\teval-mlogloss:0.24149\n",
      "[106]\ttrain-mlogloss:0.21048\teval-mlogloss:0.24137\n",
      "[107]\ttrain-mlogloss:0.20993\teval-mlogloss:0.24129\n",
      "[108]\ttrain-mlogloss:0.20950\teval-mlogloss:0.24112\n",
      "[109]\ttrain-mlogloss:0.20898\teval-mlogloss:0.24108\n",
      "[110]\ttrain-mlogloss:0.20835\teval-mlogloss:0.24117\n",
      "[111]\ttrain-mlogloss:0.20782\teval-mlogloss:0.24113\n",
      "[112]\ttrain-mlogloss:0.20736\teval-mlogloss:0.24101\n",
      "[113]\ttrain-mlogloss:0.20683\teval-mlogloss:0.24086\n",
      "[114]\ttrain-mlogloss:0.20640\teval-mlogloss:0.24074\n",
      "[115]\ttrain-mlogloss:0.20574\teval-mlogloss:0.24080\n",
      "[116]\ttrain-mlogloss:0.20516\teval-mlogloss:0.24082\n",
      "[117]\ttrain-mlogloss:0.20484\teval-mlogloss:0.24072\n",
      "[118]\ttrain-mlogloss:0.20434\teval-mlogloss:0.24072\n",
      "[119]\ttrain-mlogloss:0.20373\teval-mlogloss:0.24065\n",
      "[120]\ttrain-mlogloss:0.20322\teval-mlogloss:0.24059\n",
      "[121]\ttrain-mlogloss:0.20253\teval-mlogloss:0.24059\n",
      "[122]\ttrain-mlogloss:0.20201\teval-mlogloss:0.24052\n",
      "[123]\ttrain-mlogloss:0.20150\teval-mlogloss:0.24064\n",
      "[124]\ttrain-mlogloss:0.20094\teval-mlogloss:0.24080\n",
      "[125]\ttrain-mlogloss:0.20052\teval-mlogloss:0.24068\n",
      "[126]\ttrain-mlogloss:0.20006\teval-mlogloss:0.24062\n",
      "[127]\ttrain-mlogloss:0.19964\teval-mlogloss:0.24064\n",
      "[128]\ttrain-mlogloss:0.19923\teval-mlogloss:0.24061\n",
      "[129]\ttrain-mlogloss:0.19880\teval-mlogloss:0.24056\n",
      "[130]\ttrain-mlogloss:0.19833\teval-mlogloss:0.24077\n",
      "[131]\ttrain-mlogloss:0.19804\teval-mlogloss:0.24084\n",
      "[132]\ttrain-mlogloss:0.19757\teval-mlogloss:0.24105\n",
      "[133]\ttrain-mlogloss:0.19709\teval-mlogloss:0.24109\n",
      "[134]\ttrain-mlogloss:0.19666\teval-mlogloss:0.24116\n",
      "[135]\ttrain-mlogloss:0.19620\teval-mlogloss:0.24137\n",
      "[136]\ttrain-mlogloss:0.19574\teval-mlogloss:0.24137\n",
      "[137]\ttrain-mlogloss:0.19533\teval-mlogloss:0.24158\n",
      "[138]\ttrain-mlogloss:0.19468\teval-mlogloss:0.24170\n",
      "[139]\ttrain-mlogloss:0.19431\teval-mlogloss:0.24173\n",
      "[140]\ttrain-mlogloss:0.19399\teval-mlogloss:0.24172\n",
      "[141]\ttrain-mlogloss:0.19359\teval-mlogloss:0.24161\n",
      "[142]\ttrain-mlogloss:0.19315\teval-mlogloss:0.24179\n",
      "[143]\ttrain-mlogloss:0.19271\teval-mlogloss:0.24212\n",
      "[144]\ttrain-mlogloss:0.19250\teval-mlogloss:0.24214\n",
      "[145]\ttrain-mlogloss:0.19203\teval-mlogloss:0.24220\n",
      "[146]\ttrain-mlogloss:0.19168\teval-mlogloss:0.24214\n",
      "[147]\ttrain-mlogloss:0.19134\teval-mlogloss:0.24211\n",
      "[148]\ttrain-mlogloss:0.19109\teval-mlogloss:0.24206\n",
      "[149]\ttrain-mlogloss:0.19071\teval-mlogloss:0.24220\n",
      "[150]\ttrain-mlogloss:0.19045\teval-mlogloss:0.24215\n",
      "[151]\ttrain-mlogloss:0.19005\teval-mlogloss:0.24219\n",
      "[152]\ttrain-mlogloss:0.18963\teval-mlogloss:0.24228\n",
      "[153]\ttrain-mlogloss:0.18917\teval-mlogloss:0.24231\n",
      "[154]\ttrain-mlogloss:0.18888\teval-mlogloss:0.24231\n",
      "[155]\ttrain-mlogloss:0.18847\teval-mlogloss:0.24233\n",
      "[156]\ttrain-mlogloss:0.18814\teval-mlogloss:0.24225\n",
      "[157]\ttrain-mlogloss:0.18779\teval-mlogloss:0.24213\n",
      "[158]\ttrain-mlogloss:0.18743\teval-mlogloss:0.24232\n",
      "[159]\ttrain-mlogloss:0.18705\teval-mlogloss:0.24236\n",
      "[160]\ttrain-mlogloss:0.18671\teval-mlogloss:0.24225\n",
      "[161]\ttrain-mlogloss:0.18638\teval-mlogloss:0.24224\n",
      "[162]\ttrain-mlogloss:0.18593\teval-mlogloss:0.24228\n",
      "[163]\ttrain-mlogloss:0.18563\teval-mlogloss:0.24232\n",
      "[164]\ttrain-mlogloss:0.18526\teval-mlogloss:0.24203\n",
      "[165]\ttrain-mlogloss:0.18491\teval-mlogloss:0.24205\n",
      "[166]\ttrain-mlogloss:0.18451\teval-mlogloss:0.24215\n",
      "[167]\ttrain-mlogloss:0.18415\teval-mlogloss:0.24232\n",
      "[168]\ttrain-mlogloss:0.18370\teval-mlogloss:0.24260\n",
      "[169]\ttrain-mlogloss:0.18340\teval-mlogloss:0.24284\n",
      "[170]\ttrain-mlogloss:0.18314\teval-mlogloss:0.24281\n",
      "[171]\ttrain-mlogloss:0.18277\teval-mlogloss:0.24298\n",
      "[172]\ttrain-mlogloss:0.18249\teval-mlogloss:0.24298\n",
      "[173]\ttrain-mlogloss:0.18205\teval-mlogloss:0.24315\n",
      "[174]\ttrain-mlogloss:0.18184\teval-mlogloss:0.24327\n",
      "[175]\ttrain-mlogloss:0.18164\teval-mlogloss:0.24337\n",
      "[176]\ttrain-mlogloss:0.18132\teval-mlogloss:0.24337\n",
      "[177]\ttrain-mlogloss:0.18108\teval-mlogloss:0.24325\n",
      "[178]\ttrain-mlogloss:0.18080\teval-mlogloss:0.24317\n",
      "[179]\ttrain-mlogloss:0.18043\teval-mlogloss:0.24337\n",
      "[180]\ttrain-mlogloss:0.18014\teval-mlogloss:0.24335\n",
      "[181]\ttrain-mlogloss:0.17981\teval-mlogloss:0.24371\n",
      "[182]\ttrain-mlogloss:0.17948\teval-mlogloss:0.24378\n",
      "[183]\ttrain-mlogloss:0.17923\teval-mlogloss:0.24380\n",
      "[184]\ttrain-mlogloss:0.17886\teval-mlogloss:0.24372\n",
      "[185]\ttrain-mlogloss:0.17868\teval-mlogloss:0.24385\n",
      "[186]\ttrain-mlogloss:0.17842\teval-mlogloss:0.24397\n",
      "[187]\ttrain-mlogloss:0.17818\teval-mlogloss:0.24400\n",
      "[188]\ttrain-mlogloss:0.17793\teval-mlogloss:0.24406\n",
      "[189]\ttrain-mlogloss:0.17752\teval-mlogloss:0.24408\n",
      "[190]\ttrain-mlogloss:0.17719\teval-mlogloss:0.24420\n",
      "[191]\ttrain-mlogloss:0.17686\teval-mlogloss:0.24426\n",
      "[192]\ttrain-mlogloss:0.17656\teval-mlogloss:0.24425\n",
      "[193]\ttrain-mlogloss:0.17627\teval-mlogloss:0.24430\n",
      "[194]\ttrain-mlogloss:0.17612\teval-mlogloss:0.24427\n",
      "[195]\ttrain-mlogloss:0.17593\teval-mlogloss:0.24424\n",
      "[196]\ttrain-mlogloss:0.17569\teval-mlogloss:0.24423\n",
      "[197]\ttrain-mlogloss:0.17532\teval-mlogloss:0.24427\n",
      "[198]\ttrain-mlogloss:0.17518\teval-mlogloss:0.24440\n",
      "[199]\ttrain-mlogloss:0.17493\teval-mlogloss:0.24447\n",
      "[200]\ttrain-mlogloss:0.17475\teval-mlogloss:0.24454\n",
      "[201]\ttrain-mlogloss:0.17455\teval-mlogloss:0.24460\n",
      "[202]\ttrain-mlogloss:0.17425\teval-mlogloss:0.24461\n",
      "[203]\ttrain-mlogloss:0.17404\teval-mlogloss:0.24455\n",
      "[204]\ttrain-mlogloss:0.17372\teval-mlogloss:0.24479\n",
      "[205]\ttrain-mlogloss:0.17340\teval-mlogloss:0.24484\n",
      "[206]\ttrain-mlogloss:0.17313\teval-mlogloss:0.24497\n",
      "[207]\ttrain-mlogloss:0.17292\teval-mlogloss:0.24500\n",
      "[208]\ttrain-mlogloss:0.17270\teval-mlogloss:0.24495\n",
      "[209]\ttrain-mlogloss:0.17241\teval-mlogloss:0.24509\n",
      "[210]\ttrain-mlogloss:0.17221\teval-mlogloss:0.24514\n",
      "[211]\ttrain-mlogloss:0.17204\teval-mlogloss:0.24521\n",
      "[212]\ttrain-mlogloss:0.17181\teval-mlogloss:0.24520\n",
      "[213]\ttrain-mlogloss:0.17143\teval-mlogloss:0.24518\n",
      "[214]\ttrain-mlogloss:0.17119\teval-mlogloss:0.24510\n",
      "[215]\ttrain-mlogloss:0.17089\teval-mlogloss:0.24495\n",
      "[216]\ttrain-mlogloss:0.17074\teval-mlogloss:0.24495\n",
      "[217]\ttrain-mlogloss:0.17057\teval-mlogloss:0.24482\n",
      "[218]\ttrain-mlogloss:0.17030\teval-mlogloss:0.24491\n",
      "[219]\ttrain-mlogloss:0.17016\teval-mlogloss:0.24499\n",
      "[220]\ttrain-mlogloss:0.16996\teval-mlogloss:0.24509\n",
      "[221]\ttrain-mlogloss:0.16970\teval-mlogloss:0.24528\n",
      "xgb now score is: [2.4929980950802566]\n",
      "[0]\ttrain-mlogloss:0.67064\teval-mlogloss:0.67195\n",
      "[1]\ttrain-mlogloss:0.64923\teval-mlogloss:0.65159\n",
      "[2]\ttrain-mlogloss:0.62938\teval-mlogloss:0.63292\n",
      "[3]\ttrain-mlogloss:0.61044\teval-mlogloss:0.61526\n",
      "[4]\ttrain-mlogloss:0.59248\teval-mlogloss:0.59840\n",
      "[5]\ttrain-mlogloss:0.57534\teval-mlogloss:0.58227\n",
      "[6]\ttrain-mlogloss:0.55925\teval-mlogloss:0.56718\n",
      "[7]\ttrain-mlogloss:0.54382\teval-mlogloss:0.55286\n",
      "[8]\ttrain-mlogloss:0.52910\teval-mlogloss:0.53929\n",
      "[9]\ttrain-mlogloss:0.51521\teval-mlogloss:0.52633\n",
      "[10]\ttrain-mlogloss:0.50246\teval-mlogloss:0.51444\n",
      "[11]\ttrain-mlogloss:0.48976\teval-mlogloss:0.50253\n",
      "[12]\ttrain-mlogloss:0.47787\teval-mlogloss:0.49158\n",
      "[13]\ttrain-mlogloss:0.46651\teval-mlogloss:0.48105\n",
      "[14]\ttrain-mlogloss:0.45574\teval-mlogloss:0.47126\n",
      "[15]\ttrain-mlogloss:0.44556\teval-mlogloss:0.46188\n",
      "[16]\ttrain-mlogloss:0.43567\teval-mlogloss:0.45293\n",
      "[17]\ttrain-mlogloss:0.42624\teval-mlogloss:0.44447\n",
      "[18]\ttrain-mlogloss:0.41728\teval-mlogloss:0.43642\n",
      "[19]\ttrain-mlogloss:0.40864\teval-mlogloss:0.42860\n",
      "[20]\ttrain-mlogloss:0.40037\teval-mlogloss:0.42108\n",
      "[21]\ttrain-mlogloss:0.39249\teval-mlogloss:0.41406\n",
      "[22]\ttrain-mlogloss:0.38511\teval-mlogloss:0.40740\n",
      "[23]\ttrain-mlogloss:0.37794\teval-mlogloss:0.40110\n",
      "[24]\ttrain-mlogloss:0.37113\teval-mlogloss:0.39510\n",
      "[25]\ttrain-mlogloss:0.36460\teval-mlogloss:0.38938\n",
      "[26]\ttrain-mlogloss:0.35814\teval-mlogloss:0.38389\n",
      "[27]\ttrain-mlogloss:0.35212\teval-mlogloss:0.37864\n",
      "[28]\ttrain-mlogloss:0.34623\teval-mlogloss:0.37346\n",
      "[29]\ttrain-mlogloss:0.34064\teval-mlogloss:0.36866\n",
      "[30]\ttrain-mlogloss:0.33514\teval-mlogloss:0.36387\n",
      "[31]\ttrain-mlogloss:0.32996\teval-mlogloss:0.35958\n",
      "[32]\ttrain-mlogloss:0.32494\teval-mlogloss:0.35525\n",
      "[33]\ttrain-mlogloss:0.32020\teval-mlogloss:0.35136\n",
      "[34]\ttrain-mlogloss:0.31556\teval-mlogloss:0.34752\n",
      "[35]\ttrain-mlogloss:0.31118\teval-mlogloss:0.34412\n",
      "[36]\ttrain-mlogloss:0.30698\teval-mlogloss:0.34050\n",
      "[37]\ttrain-mlogloss:0.30296\teval-mlogloss:0.33722\n",
      "[38]\ttrain-mlogloss:0.29906\teval-mlogloss:0.33415\n",
      "[39]\ttrain-mlogloss:0.29528\teval-mlogloss:0.33108\n",
      "[40]\ttrain-mlogloss:0.29173\teval-mlogloss:0.32832\n",
      "[41]\ttrain-mlogloss:0.28828\teval-mlogloss:0.32582\n",
      "[42]\ttrain-mlogloss:0.28498\teval-mlogloss:0.32341\n",
      "[43]\ttrain-mlogloss:0.28186\teval-mlogloss:0.32103\n",
      "[44]\ttrain-mlogloss:0.27875\teval-mlogloss:0.31882\n",
      "[45]\ttrain-mlogloss:0.27582\teval-mlogloss:0.31664\n",
      "[46]\ttrain-mlogloss:0.27297\teval-mlogloss:0.31472\n",
      "[47]\ttrain-mlogloss:0.27016\teval-mlogloss:0.31273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\xgboost\\core.py:94: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\ttrain-mlogloss:0.26757\teval-mlogloss:0.31078\n",
      "[49]\ttrain-mlogloss:0.26525\teval-mlogloss:0.30897\n",
      "[50]\ttrain-mlogloss:0.26269\teval-mlogloss:0.30748\n",
      "[51]\ttrain-mlogloss:0.26036\teval-mlogloss:0.30592\n",
      "[52]\ttrain-mlogloss:0.25804\teval-mlogloss:0.30432\n",
      "[53]\ttrain-mlogloss:0.25570\teval-mlogloss:0.30289\n",
      "[54]\ttrain-mlogloss:0.25356\teval-mlogloss:0.30144\n",
      "[55]\ttrain-mlogloss:0.25158\teval-mlogloss:0.30003\n",
      "[56]\ttrain-mlogloss:0.24967\teval-mlogloss:0.29886\n",
      "[57]\ttrain-mlogloss:0.24786\teval-mlogloss:0.29759\n",
      "[58]\ttrain-mlogloss:0.24605\teval-mlogloss:0.29630\n",
      "[59]\ttrain-mlogloss:0.24428\teval-mlogloss:0.29523\n",
      "[60]\ttrain-mlogloss:0.24254\teval-mlogloss:0.29432\n",
      "[61]\ttrain-mlogloss:0.24092\teval-mlogloss:0.29341\n",
      "[62]\ttrain-mlogloss:0.23925\teval-mlogloss:0.29260\n",
      "[63]\ttrain-mlogloss:0.23782\teval-mlogloss:0.29187\n",
      "[64]\ttrain-mlogloss:0.23645\teval-mlogloss:0.29124\n",
      "[65]\ttrain-mlogloss:0.23501\teval-mlogloss:0.29050\n",
      "[66]\ttrain-mlogloss:0.23347\teval-mlogloss:0.28963\n",
      "[67]\ttrain-mlogloss:0.23214\teval-mlogloss:0.28915\n",
      "[68]\ttrain-mlogloss:0.23085\teval-mlogloss:0.28862\n",
      "[69]\ttrain-mlogloss:0.22945\teval-mlogloss:0.28793\n",
      "[70]\ttrain-mlogloss:0.22800\teval-mlogloss:0.28729\n",
      "[71]\ttrain-mlogloss:0.22688\teval-mlogloss:0.28662\n",
      "[72]\ttrain-mlogloss:0.22573\teval-mlogloss:0.28607\n",
      "[73]\ttrain-mlogloss:0.22464\teval-mlogloss:0.28574\n",
      "[74]\ttrain-mlogloss:0.22351\teval-mlogloss:0.28540\n",
      "[75]\ttrain-mlogloss:0.22240\teval-mlogloss:0.28503\n",
      "[76]\ttrain-mlogloss:0.22144\teval-mlogloss:0.28492\n",
      "[77]\ttrain-mlogloss:0.22053\teval-mlogloss:0.28471\n",
      "[78]\ttrain-mlogloss:0.21942\teval-mlogloss:0.28454\n",
      "[79]\ttrain-mlogloss:0.21834\teval-mlogloss:0.28429\n",
      "[80]\ttrain-mlogloss:0.21732\teval-mlogloss:0.28422\n",
      "[81]\ttrain-mlogloss:0.21627\teval-mlogloss:0.28407\n",
      "[82]\ttrain-mlogloss:0.21537\teval-mlogloss:0.28386\n",
      "[83]\ttrain-mlogloss:0.21438\teval-mlogloss:0.28378\n",
      "[84]\ttrain-mlogloss:0.21362\teval-mlogloss:0.28365\n",
      "[85]\ttrain-mlogloss:0.21272\teval-mlogloss:0.28375\n",
      "[86]\ttrain-mlogloss:0.21200\teval-mlogloss:0.28378\n",
      "[87]\ttrain-mlogloss:0.21120\teval-mlogloss:0.28381\n",
      "[88]\ttrain-mlogloss:0.21044\teval-mlogloss:0.28385\n",
      "[89]\ttrain-mlogloss:0.20956\teval-mlogloss:0.28406\n",
      "[90]\ttrain-mlogloss:0.20866\teval-mlogloss:0.28394\n",
      "[91]\ttrain-mlogloss:0.20789\teval-mlogloss:0.28389\n",
      "[92]\ttrain-mlogloss:0.20709\teval-mlogloss:0.28388\n",
      "[93]\ttrain-mlogloss:0.20620\teval-mlogloss:0.28375\n",
      "[94]\ttrain-mlogloss:0.20542\teval-mlogloss:0.28375\n",
      "[95]\ttrain-mlogloss:0.20458\teval-mlogloss:0.28369\n",
      "[96]\ttrain-mlogloss:0.20384\teval-mlogloss:0.28384\n",
      "[97]\ttrain-mlogloss:0.20300\teval-mlogloss:0.28399\n",
      "[98]\ttrain-mlogloss:0.20240\teval-mlogloss:0.28400\n",
      "[99]\ttrain-mlogloss:0.20171\teval-mlogloss:0.28401\n",
      "[100]\ttrain-mlogloss:0.20095\teval-mlogloss:0.28419\n",
      "[101]\ttrain-mlogloss:0.20031\teval-mlogloss:0.28426\n",
      "[102]\ttrain-mlogloss:0.19948\teval-mlogloss:0.28441\n",
      "[103]\ttrain-mlogloss:0.19899\teval-mlogloss:0.28469\n",
      "[104]\ttrain-mlogloss:0.19833\teval-mlogloss:0.28475\n",
      "[105]\ttrain-mlogloss:0.19781\teval-mlogloss:0.28496\n",
      "[106]\ttrain-mlogloss:0.19713\teval-mlogloss:0.28492\n",
      "[107]\ttrain-mlogloss:0.19649\teval-mlogloss:0.28522\n",
      "[108]\ttrain-mlogloss:0.19575\teval-mlogloss:0.28534\n",
      "[109]\ttrain-mlogloss:0.19507\teval-mlogloss:0.28552\n",
      "[110]\ttrain-mlogloss:0.19458\teval-mlogloss:0.28573\n",
      "[111]\ttrain-mlogloss:0.19403\teval-mlogloss:0.28590\n",
      "[112]\ttrain-mlogloss:0.19358\teval-mlogloss:0.28584\n",
      "[113]\ttrain-mlogloss:0.19290\teval-mlogloss:0.28601\n",
      "[114]\ttrain-mlogloss:0.19244\teval-mlogloss:0.28632\n",
      "[115]\ttrain-mlogloss:0.19187\teval-mlogloss:0.28643\n",
      "[116]\ttrain-mlogloss:0.19129\teval-mlogloss:0.28663\n",
      "[117]\ttrain-mlogloss:0.19081\teval-mlogloss:0.28680\n",
      "[118]\ttrain-mlogloss:0.19034\teval-mlogloss:0.28705\n",
      "[119]\ttrain-mlogloss:0.18977\teval-mlogloss:0.28713\n",
      "[120]\ttrain-mlogloss:0.18921\teval-mlogloss:0.28761\n",
      "[121]\ttrain-mlogloss:0.18872\teval-mlogloss:0.28795\n",
      "[122]\ttrain-mlogloss:0.18804\teval-mlogloss:0.28822\n",
      "[123]\ttrain-mlogloss:0.18744\teval-mlogloss:0.28869\n",
      "[124]\ttrain-mlogloss:0.18694\teval-mlogloss:0.28888\n",
      "[125]\ttrain-mlogloss:0.18642\teval-mlogloss:0.28906\n",
      "[126]\ttrain-mlogloss:0.18586\teval-mlogloss:0.28931\n",
      "[127]\ttrain-mlogloss:0.18544\teval-mlogloss:0.28964\n",
      "[128]\ttrain-mlogloss:0.18504\teval-mlogloss:0.28969\n",
      "[129]\ttrain-mlogloss:0.18456\teval-mlogloss:0.28987\n",
      "[130]\ttrain-mlogloss:0.18402\teval-mlogloss:0.29015\n",
      "[131]\ttrain-mlogloss:0.18361\teval-mlogloss:0.29037\n",
      "[132]\ttrain-mlogloss:0.18310\teval-mlogloss:0.29067\n",
      "[133]\ttrain-mlogloss:0.18258\teval-mlogloss:0.29091\n",
      "[134]\ttrain-mlogloss:0.18223\teval-mlogloss:0.29105\n",
      "[135]\ttrain-mlogloss:0.18179\teval-mlogloss:0.29129\n",
      "[136]\ttrain-mlogloss:0.18132\teval-mlogloss:0.29140\n",
      "[137]\ttrain-mlogloss:0.18081\teval-mlogloss:0.29177\n",
      "[138]\ttrain-mlogloss:0.18038\teval-mlogloss:0.29195\n",
      "[139]\ttrain-mlogloss:0.17987\teval-mlogloss:0.29219\n",
      "[140]\ttrain-mlogloss:0.17936\teval-mlogloss:0.29247\n",
      "[141]\ttrain-mlogloss:0.17898\teval-mlogloss:0.29251\n",
      "[142]\ttrain-mlogloss:0.17834\teval-mlogloss:0.29274\n",
      "[143]\ttrain-mlogloss:0.17786\teval-mlogloss:0.29321\n",
      "[144]\ttrain-mlogloss:0.17745\teval-mlogloss:0.29320\n",
      "[145]\ttrain-mlogloss:0.17707\teval-mlogloss:0.29349\n",
      "[146]\ttrain-mlogloss:0.17665\teval-mlogloss:0.29361\n",
      "[147]\ttrain-mlogloss:0.17620\teval-mlogloss:0.29378\n",
      "[148]\ttrain-mlogloss:0.17576\teval-mlogloss:0.29406\n",
      "[149]\ttrain-mlogloss:0.17539\teval-mlogloss:0.29427\n",
      "[150]\ttrain-mlogloss:0.17497\teval-mlogloss:0.29459\n",
      "[151]\ttrain-mlogloss:0.17456\teval-mlogloss:0.29465\n",
      "[152]\ttrain-mlogloss:0.17424\teval-mlogloss:0.29503\n",
      "[153]\ttrain-mlogloss:0.17380\teval-mlogloss:0.29545\n",
      "[154]\ttrain-mlogloss:0.17352\teval-mlogloss:0.29563\n",
      "[155]\ttrain-mlogloss:0.17321\teval-mlogloss:0.29586\n",
      "[156]\ttrain-mlogloss:0.17285\teval-mlogloss:0.29592\n",
      "[157]\ttrain-mlogloss:0.17242\teval-mlogloss:0.29611\n",
      "[158]\ttrain-mlogloss:0.17197\teval-mlogloss:0.29630\n",
      "[159]\ttrain-mlogloss:0.17156\teval-mlogloss:0.29647\n",
      "[160]\ttrain-mlogloss:0.17121\teval-mlogloss:0.29660\n",
      "[161]\ttrain-mlogloss:0.17068\teval-mlogloss:0.29689\n",
      "[162]\ttrain-mlogloss:0.17036\teval-mlogloss:0.29722\n",
      "[163]\ttrain-mlogloss:0.17007\teval-mlogloss:0.29757\n",
      "[164]\ttrain-mlogloss:0.16978\teval-mlogloss:0.29766\n",
      "[165]\ttrain-mlogloss:0.16945\teval-mlogloss:0.29804\n",
      "[166]\ttrain-mlogloss:0.16907\teval-mlogloss:0.29850\n",
      "[167]\ttrain-mlogloss:0.16865\teval-mlogloss:0.29885\n",
      "[168]\ttrain-mlogloss:0.16824\teval-mlogloss:0.29888\n",
      "[169]\ttrain-mlogloss:0.16794\teval-mlogloss:0.29929\n",
      "[170]\ttrain-mlogloss:0.16775\teval-mlogloss:0.29925\n",
      "[171]\ttrain-mlogloss:0.16747\teval-mlogloss:0.29946\n",
      "[172]\ttrain-mlogloss:0.16718\teval-mlogloss:0.29940\n",
      "[173]\ttrain-mlogloss:0.16686\teval-mlogloss:0.29951\n",
      "[174]\ttrain-mlogloss:0.16655\teval-mlogloss:0.29973\n",
      "[175]\ttrain-mlogloss:0.16636\teval-mlogloss:0.29980\n",
      "[176]\ttrain-mlogloss:0.16611\teval-mlogloss:0.30013\n",
      "[177]\ttrain-mlogloss:0.16572\teval-mlogloss:0.30045\n",
      "[178]\ttrain-mlogloss:0.16538\teval-mlogloss:0.30070\n",
      "[179]\ttrain-mlogloss:0.16507\teval-mlogloss:0.30090\n",
      "[180]\ttrain-mlogloss:0.16491\teval-mlogloss:0.30088\n",
      "[181]\ttrain-mlogloss:0.16460\teval-mlogloss:0.30087\n",
      "[182]\ttrain-mlogloss:0.16426\teval-mlogloss:0.30117\n",
      "[183]\ttrain-mlogloss:0.16405\teval-mlogloss:0.30116\n",
      "[184]\ttrain-mlogloss:0.16374\teval-mlogloss:0.30121\n",
      "xgb now score is: [2.4929980950802566, 2.1850327644497156]\n",
      "[0]\ttrain-mlogloss:0.67063\teval-mlogloss:0.67126\n",
      "[1]\ttrain-mlogloss:0.64954\teval-mlogloss:0.65067\n",
      "[2]\ttrain-mlogloss:0.62974\teval-mlogloss:0.63171\n",
      "[3]\ttrain-mlogloss:0.61109\teval-mlogloss:0.61378\n",
      "[4]\ttrain-mlogloss:0.59333\teval-mlogloss:0.59670\n",
      "[5]\ttrain-mlogloss:0.57629\teval-mlogloss:0.58043\n",
      "[6]\ttrain-mlogloss:0.56047\teval-mlogloss:0.56519\n",
      "[7]\ttrain-mlogloss:0.54504\teval-mlogloss:0.55027\n",
      "[8]\ttrain-mlogloss:0.53033\teval-mlogloss:0.53630\n",
      "[9]\ttrain-mlogloss:0.51657\teval-mlogloss:0.52309\n",
      "[10]\ttrain-mlogloss:0.50352\teval-mlogloss:0.51066\n",
      "[11]\ttrain-mlogloss:0.49106\teval-mlogloss:0.49875\n",
      "[12]\ttrain-mlogloss:0.47942\teval-mlogloss:0.48761\n",
      "[13]\ttrain-mlogloss:0.46804\teval-mlogloss:0.47690\n",
      "[14]\ttrain-mlogloss:0.45730\teval-mlogloss:0.46654\n",
      "[15]\ttrain-mlogloss:0.44689\teval-mlogloss:0.45674\n",
      "[16]\ttrain-mlogloss:0.43697\teval-mlogloss:0.44747\n",
      "[17]\ttrain-mlogloss:0.42751\teval-mlogloss:0.43865\n",
      "[18]\ttrain-mlogloss:0.41863\teval-mlogloss:0.43029\n",
      "[19]\ttrain-mlogloss:0.41015\teval-mlogloss:0.42232\n",
      "[20]\ttrain-mlogloss:0.40195\teval-mlogloss:0.41464\n",
      "[21]\ttrain-mlogloss:0.39421\teval-mlogloss:0.40755\n",
      "[22]\ttrain-mlogloss:0.38669\teval-mlogloss:0.40057\n",
      "[23]\ttrain-mlogloss:0.37966\teval-mlogloss:0.39408\n",
      "[24]\ttrain-mlogloss:0.37286\teval-mlogloss:0.38772\n",
      "[25]\ttrain-mlogloss:0.36625\teval-mlogloss:0.38160\n",
      "[26]\ttrain-mlogloss:0.35998\teval-mlogloss:0.37569\n",
      "[27]\ttrain-mlogloss:0.35394\teval-mlogloss:0.37018\n",
      "[28]\ttrain-mlogloss:0.34806\teval-mlogloss:0.36483\n",
      "[29]\ttrain-mlogloss:0.34259\teval-mlogloss:0.35985\n",
      "[30]\ttrain-mlogloss:0.33709\teval-mlogloss:0.35497\n",
      "[31]\ttrain-mlogloss:0.33207\teval-mlogloss:0.35046\n",
      "[32]\ttrain-mlogloss:0.32717\teval-mlogloss:0.34611\n",
      "[33]\ttrain-mlogloss:0.32257\teval-mlogloss:0.34214\n",
      "[34]\ttrain-mlogloss:0.31794\teval-mlogloss:0.33798\n",
      "[35]\ttrain-mlogloss:0.31366\teval-mlogloss:0.33420\n",
      "[36]\ttrain-mlogloss:0.30945\teval-mlogloss:0.33049\n",
      "[37]\ttrain-mlogloss:0.30529\teval-mlogloss:0.32705\n",
      "[38]\ttrain-mlogloss:0.30128\teval-mlogloss:0.32374\n",
      "[39]\ttrain-mlogloss:0.29763\teval-mlogloss:0.32059\n",
      "[40]\ttrain-mlogloss:0.29413\teval-mlogloss:0.31756\n",
      "[41]\ttrain-mlogloss:0.29057\teval-mlogloss:0.31480\n",
      "[42]\ttrain-mlogloss:0.28704\teval-mlogloss:0.31201\n",
      "[43]\ttrain-mlogloss:0.28388\teval-mlogloss:0.30943\n",
      "[44]\ttrain-mlogloss:0.28093\teval-mlogloss:0.30701\n",
      "[45]\ttrain-mlogloss:0.27804\teval-mlogloss:0.30469\n",
      "[46]\ttrain-mlogloss:0.27531\teval-mlogloss:0.30251\n",
      "[47]\ttrain-mlogloss:0.27258\teval-mlogloss:0.30036\n",
      "[48]\ttrain-mlogloss:0.26972\teval-mlogloss:0.29824\n",
      "[49]\ttrain-mlogloss:0.26702\teval-mlogloss:0.29622\n",
      "[50]\ttrain-mlogloss:0.26449\teval-mlogloss:0.29431\n",
      "[51]\ttrain-mlogloss:0.26220\teval-mlogloss:0.29257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\xgboost\\core.py:94: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52]\ttrain-mlogloss:0.25997\teval-mlogloss:0.29075\n",
      "[53]\ttrain-mlogloss:0.25767\teval-mlogloss:0.28912\n",
      "[54]\ttrain-mlogloss:0.25543\teval-mlogloss:0.28753\n",
      "[55]\ttrain-mlogloss:0.25333\teval-mlogloss:0.28607\n",
      "[56]\ttrain-mlogloss:0.25137\teval-mlogloss:0.28460\n",
      "[57]\ttrain-mlogloss:0.24945\teval-mlogloss:0.28331\n",
      "[58]\ttrain-mlogloss:0.24752\teval-mlogloss:0.28203\n",
      "[59]\ttrain-mlogloss:0.24551\teval-mlogloss:0.28073\n",
      "[60]\ttrain-mlogloss:0.24369\teval-mlogloss:0.27969\n",
      "[61]\ttrain-mlogloss:0.24205\teval-mlogloss:0.27873\n",
      "[62]\ttrain-mlogloss:0.24046\teval-mlogloss:0.27771\n",
      "[63]\ttrain-mlogloss:0.23904\teval-mlogloss:0.27672\n",
      "[64]\ttrain-mlogloss:0.23752\teval-mlogloss:0.27568\n",
      "[65]\ttrain-mlogloss:0.23612\teval-mlogloss:0.27478\n",
      "[66]\ttrain-mlogloss:0.23472\teval-mlogloss:0.27391\n",
      "[67]\ttrain-mlogloss:0.23326\teval-mlogloss:0.27316\n",
      "[68]\ttrain-mlogloss:0.23198\teval-mlogloss:0.27254\n",
      "[69]\ttrain-mlogloss:0.23072\teval-mlogloss:0.27188\n",
      "[70]\ttrain-mlogloss:0.22942\teval-mlogloss:0.27116\n",
      "[71]\ttrain-mlogloss:0.22816\teval-mlogloss:0.27054\n",
      "[72]\ttrain-mlogloss:0.22703\teval-mlogloss:0.26996\n",
      "[73]\ttrain-mlogloss:0.22593\teval-mlogloss:0.26945\n",
      "[74]\ttrain-mlogloss:0.22479\teval-mlogloss:0.26891\n",
      "[75]\ttrain-mlogloss:0.22375\teval-mlogloss:0.26846\n",
      "[76]\ttrain-mlogloss:0.22257\teval-mlogloss:0.26792\n",
      "[77]\ttrain-mlogloss:0.22162\teval-mlogloss:0.26747\n",
      "[78]\ttrain-mlogloss:0.22067\teval-mlogloss:0.26702\n",
      "[79]\ttrain-mlogloss:0.21979\teval-mlogloss:0.26658\n",
      "[80]\ttrain-mlogloss:0.21902\teval-mlogloss:0.26622\n",
      "[81]\ttrain-mlogloss:0.21825\teval-mlogloss:0.26575\n",
      "[82]\ttrain-mlogloss:0.21735\teval-mlogloss:0.26538\n",
      "[83]\ttrain-mlogloss:0.21632\teval-mlogloss:0.26516\n",
      "[84]\ttrain-mlogloss:0.21557\teval-mlogloss:0.26496\n",
      "[85]\ttrain-mlogloss:0.21479\teval-mlogloss:0.26478\n",
      "[86]\ttrain-mlogloss:0.21376\teval-mlogloss:0.26456\n",
      "[87]\ttrain-mlogloss:0.21310\teval-mlogloss:0.26427\n",
      "[88]\ttrain-mlogloss:0.21219\teval-mlogloss:0.26412\n",
      "[89]\ttrain-mlogloss:0.21126\teval-mlogloss:0.26384\n",
      "[90]\ttrain-mlogloss:0.21049\teval-mlogloss:0.26368\n",
      "[91]\ttrain-mlogloss:0.20991\teval-mlogloss:0.26360\n",
      "[92]\ttrain-mlogloss:0.20914\teval-mlogloss:0.26373\n",
      "[93]\ttrain-mlogloss:0.20843\teval-mlogloss:0.26369\n",
      "[94]\ttrain-mlogloss:0.20769\teval-mlogloss:0.26340\n",
      "[95]\ttrain-mlogloss:0.20701\teval-mlogloss:0.26323\n",
      "[96]\ttrain-mlogloss:0.20629\teval-mlogloss:0.26324\n",
      "[97]\ttrain-mlogloss:0.20567\teval-mlogloss:0.26319\n",
      "[98]\ttrain-mlogloss:0.20506\teval-mlogloss:0.26315\n",
      "[99]\ttrain-mlogloss:0.20451\teval-mlogloss:0.26292\n",
      "[100]\ttrain-mlogloss:0.20389\teval-mlogloss:0.26278\n",
      "[101]\ttrain-mlogloss:0.20322\teval-mlogloss:0.26292\n",
      "[102]\ttrain-mlogloss:0.20263\teval-mlogloss:0.26283\n",
      "[103]\ttrain-mlogloss:0.20202\teval-mlogloss:0.26298\n",
      "[104]\ttrain-mlogloss:0.20164\teval-mlogloss:0.26301\n",
      "[105]\ttrain-mlogloss:0.20099\teval-mlogloss:0.26311\n",
      "[106]\ttrain-mlogloss:0.20059\teval-mlogloss:0.26305\n",
      "[107]\ttrain-mlogloss:0.20013\teval-mlogloss:0.26302\n",
      "[108]\ttrain-mlogloss:0.19939\teval-mlogloss:0.26298\n",
      "[109]\ttrain-mlogloss:0.19891\teval-mlogloss:0.26317\n",
      "[110]\ttrain-mlogloss:0.19832\teval-mlogloss:0.26326\n",
      "[111]\ttrain-mlogloss:0.19778\teval-mlogloss:0.26340\n",
      "[112]\ttrain-mlogloss:0.19725\teval-mlogloss:0.26343\n",
      "[113]\ttrain-mlogloss:0.19677\teval-mlogloss:0.26344\n",
      "[114]\ttrain-mlogloss:0.19621\teval-mlogloss:0.26368\n",
      "[115]\ttrain-mlogloss:0.19560\teval-mlogloss:0.26381\n",
      "[116]\ttrain-mlogloss:0.19514\teval-mlogloss:0.26390\n",
      "[117]\ttrain-mlogloss:0.19466\teval-mlogloss:0.26410\n",
      "[118]\ttrain-mlogloss:0.19418\teval-mlogloss:0.26437\n",
      "[119]\ttrain-mlogloss:0.19378\teval-mlogloss:0.26443\n",
      "[120]\ttrain-mlogloss:0.19326\teval-mlogloss:0.26462\n",
      "[121]\ttrain-mlogloss:0.19266\teval-mlogloss:0.26489\n",
      "[122]\ttrain-mlogloss:0.19214\teval-mlogloss:0.26490\n",
      "[123]\ttrain-mlogloss:0.19162\teval-mlogloss:0.26504\n",
      "[124]\ttrain-mlogloss:0.19123\teval-mlogloss:0.26504\n",
      "[125]\ttrain-mlogloss:0.19073\teval-mlogloss:0.26523\n",
      "[126]\ttrain-mlogloss:0.19032\teval-mlogloss:0.26537\n",
      "[127]\ttrain-mlogloss:0.18989\teval-mlogloss:0.26528\n",
      "[128]\ttrain-mlogloss:0.18945\teval-mlogloss:0.26548\n",
      "[129]\ttrain-mlogloss:0.18911\teval-mlogloss:0.26568\n",
      "[130]\ttrain-mlogloss:0.18871\teval-mlogloss:0.26578\n",
      "[131]\ttrain-mlogloss:0.18821\teval-mlogloss:0.26601\n",
      "[132]\ttrain-mlogloss:0.18785\teval-mlogloss:0.26635\n",
      "[133]\ttrain-mlogloss:0.18748\teval-mlogloss:0.26639\n",
      "[134]\ttrain-mlogloss:0.18695\teval-mlogloss:0.26628\n",
      "[135]\ttrain-mlogloss:0.18650\teval-mlogloss:0.26652\n",
      "[136]\ttrain-mlogloss:0.18614\teval-mlogloss:0.26654\n",
      "[137]\ttrain-mlogloss:0.18588\teval-mlogloss:0.26663\n",
      "[138]\ttrain-mlogloss:0.18552\teval-mlogloss:0.26677\n",
      "[139]\ttrain-mlogloss:0.18518\teval-mlogloss:0.26680\n",
      "[140]\ttrain-mlogloss:0.18466\teval-mlogloss:0.26691\n",
      "[141]\ttrain-mlogloss:0.18443\teval-mlogloss:0.26695\n",
      "[142]\ttrain-mlogloss:0.18410\teval-mlogloss:0.26686\n",
      "[143]\ttrain-mlogloss:0.18375\teval-mlogloss:0.26684\n",
      "[144]\ttrain-mlogloss:0.18331\teval-mlogloss:0.26677\n",
      "[145]\ttrain-mlogloss:0.18293\teval-mlogloss:0.26678\n",
      "[146]\ttrain-mlogloss:0.18257\teval-mlogloss:0.26712\n",
      "[147]\ttrain-mlogloss:0.18221\teval-mlogloss:0.26739\n",
      "[148]\ttrain-mlogloss:0.18182\teval-mlogloss:0.26753\n",
      "[149]\ttrain-mlogloss:0.18146\teval-mlogloss:0.26788\n",
      "[150]\ttrain-mlogloss:0.18103\teval-mlogloss:0.26813\n",
      "[151]\ttrain-mlogloss:0.18060\teval-mlogloss:0.26818\n",
      "[152]\ttrain-mlogloss:0.18027\teval-mlogloss:0.26848\n",
      "[153]\ttrain-mlogloss:0.17988\teval-mlogloss:0.26867\n",
      "[154]\ttrain-mlogloss:0.17960\teval-mlogloss:0.26869\n",
      "[155]\ttrain-mlogloss:0.17921\teval-mlogloss:0.26865\n",
      "[156]\ttrain-mlogloss:0.17888\teval-mlogloss:0.26862\n",
      "[157]\ttrain-mlogloss:0.17863\teval-mlogloss:0.26886\n",
      "[158]\ttrain-mlogloss:0.17823\teval-mlogloss:0.26898\n",
      "[159]\ttrain-mlogloss:0.17778\teval-mlogloss:0.26906\n",
      "[160]\ttrain-mlogloss:0.17752\teval-mlogloss:0.26915\n",
      "[161]\ttrain-mlogloss:0.17716\teval-mlogloss:0.26929\n",
      "[162]\ttrain-mlogloss:0.17678\teval-mlogloss:0.26920\n",
      "[163]\ttrain-mlogloss:0.17636\teval-mlogloss:0.26953\n",
      "[164]\ttrain-mlogloss:0.17603\teval-mlogloss:0.26956\n",
      "[165]\ttrain-mlogloss:0.17565\teval-mlogloss:0.26990\n",
      "[166]\ttrain-mlogloss:0.17527\teval-mlogloss:0.26999\n",
      "[167]\ttrain-mlogloss:0.17507\teval-mlogloss:0.26993\n",
      "[168]\ttrain-mlogloss:0.17482\teval-mlogloss:0.27014\n",
      "[169]\ttrain-mlogloss:0.17465\teval-mlogloss:0.27017\n",
      "[170]\ttrain-mlogloss:0.17427\teval-mlogloss:0.27019\n",
      "[171]\ttrain-mlogloss:0.17377\teval-mlogloss:0.27042\n",
      "[172]\ttrain-mlogloss:0.17361\teval-mlogloss:0.27044\n",
      "[173]\ttrain-mlogloss:0.17332\teval-mlogloss:0.27045\n",
      "[174]\ttrain-mlogloss:0.17305\teval-mlogloss:0.27058\n",
      "[175]\ttrain-mlogloss:0.17272\teval-mlogloss:0.27073\n",
      "[176]\ttrain-mlogloss:0.17238\teval-mlogloss:0.27087\n",
      "[177]\ttrain-mlogloss:0.17205\teval-mlogloss:0.27088\n",
      "[178]\ttrain-mlogloss:0.17179\teval-mlogloss:0.27093\n",
      "[179]\ttrain-mlogloss:0.17142\teval-mlogloss:0.27082\n",
      "[180]\ttrain-mlogloss:0.17111\teval-mlogloss:0.27077\n",
      "[181]\ttrain-mlogloss:0.17084\teval-mlogloss:0.27093\n",
      "[182]\ttrain-mlogloss:0.17043\teval-mlogloss:0.27081\n",
      "[183]\ttrain-mlogloss:0.17020\teval-mlogloss:0.27095\n",
      "[184]\ttrain-mlogloss:0.16997\teval-mlogloss:0.27104\n",
      "[185]\ttrain-mlogloss:0.16965\teval-mlogloss:0.27133\n",
      "[186]\ttrain-mlogloss:0.16934\teval-mlogloss:0.27156\n",
      "[187]\ttrain-mlogloss:0.16899\teval-mlogloss:0.27169\n",
      "[188]\ttrain-mlogloss:0.16866\teval-mlogloss:0.27174\n",
      "[189]\ttrain-mlogloss:0.16839\teval-mlogloss:0.27199\n",
      "[190]\ttrain-mlogloss:0.16813\teval-mlogloss:0.27209\n",
      "[191]\ttrain-mlogloss:0.16789\teval-mlogloss:0.27221\n",
      "[192]\ttrain-mlogloss:0.16761\teval-mlogloss:0.27249\n",
      "[193]\ttrain-mlogloss:0.16738\teval-mlogloss:0.27270\n",
      "[194]\ttrain-mlogloss:0.16718\teval-mlogloss:0.27281\n",
      "[195]\ttrain-mlogloss:0.16690\teval-mlogloss:0.27301\n",
      "[196]\ttrain-mlogloss:0.16655\teval-mlogloss:0.27318\n",
      "[197]\ttrain-mlogloss:0.16617\teval-mlogloss:0.27330\n",
      "[198]\ttrain-mlogloss:0.16591\teval-mlogloss:0.27349\n",
      "[199]\ttrain-mlogloss:0.16575\teval-mlogloss:0.27359\n",
      "xgb now score is: [2.4929980950802566, 2.1850327644497156, 2.371474780375138]\n",
      "[0]\ttrain-mlogloss:0.67120\teval-mlogloss:0.67089\n",
      "[1]\ttrain-mlogloss:0.65046\teval-mlogloss:0.64967\n",
      "[2]\ttrain-mlogloss:0.63091\teval-mlogloss:0.62976\n",
      "[3]\ttrain-mlogloss:0.61256\teval-mlogloss:0.61090\n",
      "[4]\ttrain-mlogloss:0.59525\teval-mlogloss:0.59302\n",
      "[5]\ttrain-mlogloss:0.57876\teval-mlogloss:0.57610\n",
      "[6]\ttrain-mlogloss:0.56332\teval-mlogloss:0.56019\n",
      "[7]\ttrain-mlogloss:0.54841\teval-mlogloss:0.54480\n",
      "[8]\ttrain-mlogloss:0.53427\teval-mlogloss:0.53005\n",
      "[9]\ttrain-mlogloss:0.52085\teval-mlogloss:0.51621\n",
      "[10]\ttrain-mlogloss:0.50823\teval-mlogloss:0.50343\n",
      "[11]\ttrain-mlogloss:0.49614\teval-mlogloss:0.49106\n",
      "[12]\ttrain-mlogloss:0.48451\teval-mlogloss:0.47901\n",
      "[13]\ttrain-mlogloss:0.47359\teval-mlogloss:0.46773\n",
      "[14]\ttrain-mlogloss:0.46308\teval-mlogloss:0.45699\n",
      "[15]\ttrain-mlogloss:0.45319\teval-mlogloss:0.44665\n",
      "[16]\ttrain-mlogloss:0.44368\teval-mlogloss:0.43685\n",
      "[17]\ttrain-mlogloss:0.43461\teval-mlogloss:0.42735\n",
      "[18]\ttrain-mlogloss:0.42591\teval-mlogloss:0.41833\n",
      "[19]\ttrain-mlogloss:0.41764\teval-mlogloss:0.40980\n",
      "[20]\ttrain-mlogloss:0.40973\teval-mlogloss:0.40150\n",
      "[21]\ttrain-mlogloss:0.40206\teval-mlogloss:0.39348\n",
      "[22]\ttrain-mlogloss:0.39484\teval-mlogloss:0.38601\n",
      "[23]\ttrain-mlogloss:0.38772\teval-mlogloss:0.37860\n",
      "[24]\ttrain-mlogloss:0.38113\teval-mlogloss:0.37169\n",
      "[25]\ttrain-mlogloss:0.37462\teval-mlogloss:0.36512\n",
      "[26]\ttrain-mlogloss:0.36858\teval-mlogloss:0.35887\n",
      "[27]\ttrain-mlogloss:0.36270\teval-mlogloss:0.35284\n",
      "[28]\ttrain-mlogloss:0.35708\teval-mlogloss:0.34705\n",
      "[29]\ttrain-mlogloss:0.35185\teval-mlogloss:0.34149\n",
      "[30]\ttrain-mlogloss:0.34675\teval-mlogloss:0.33619\n",
      "[31]\ttrain-mlogloss:0.34186\teval-mlogloss:0.33099\n",
      "[32]\ttrain-mlogloss:0.33714\teval-mlogloss:0.32606\n",
      "[33]\ttrain-mlogloss:0.33256\teval-mlogloss:0.32139\n",
      "[34]\ttrain-mlogloss:0.32821\teval-mlogloss:0.31690\n",
      "[35]\ttrain-mlogloss:0.32398\teval-mlogloss:0.31255\n",
      "[36]\ttrain-mlogloss:0.31992\teval-mlogloss:0.30842\n",
      "[37]\ttrain-mlogloss:0.31619\teval-mlogloss:0.30466\n",
      "[38]\ttrain-mlogloss:0.31248\teval-mlogloss:0.30094\n",
      "[39]\ttrain-mlogloss:0.30888\teval-mlogloss:0.29731\n",
      "[40]\ttrain-mlogloss:0.30543\teval-mlogloss:0.29377\n",
      "[41]\ttrain-mlogloss:0.30210\teval-mlogloss:0.29031\n",
      "[42]\ttrain-mlogloss:0.29895\teval-mlogloss:0.28697\n",
      "[43]\ttrain-mlogloss:0.29595\teval-mlogloss:0.28386\n",
      "[44]\ttrain-mlogloss:0.29303\teval-mlogloss:0.28077\n",
      "[45]\ttrain-mlogloss:0.29012\teval-mlogloss:0.27779\n",
      "[46]\ttrain-mlogloss:0.28737\teval-mlogloss:0.27491\n",
      "[47]\ttrain-mlogloss:0.28474\teval-mlogloss:0.27234\n",
      "[48]\ttrain-mlogloss:0.28231\teval-mlogloss:0.26977\n",
      "[49]\ttrain-mlogloss:0.28004\teval-mlogloss:0.26744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\xgboost\\core.py:94: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain-mlogloss:0.27775\teval-mlogloss:0.26501\n",
      "[51]\ttrain-mlogloss:0.27538\teval-mlogloss:0.26276\n",
      "[52]\ttrain-mlogloss:0.27324\teval-mlogloss:0.26039\n",
      "[53]\ttrain-mlogloss:0.27093\teval-mlogloss:0.25819\n",
      "[54]\ttrain-mlogloss:0.26904\teval-mlogloss:0.25623\n",
      "[55]\ttrain-mlogloss:0.26713\teval-mlogloss:0.25428\n",
      "[56]\ttrain-mlogloss:0.26535\teval-mlogloss:0.25245\n",
      "[57]\ttrain-mlogloss:0.26350\teval-mlogloss:0.25066\n",
      "[58]\ttrain-mlogloss:0.26165\teval-mlogloss:0.24868\n",
      "[59]\ttrain-mlogloss:0.25990\teval-mlogloss:0.24695\n",
      "[60]\ttrain-mlogloss:0.25825\teval-mlogloss:0.24535\n",
      "[61]\ttrain-mlogloss:0.25670\teval-mlogloss:0.24385\n",
      "[62]\ttrain-mlogloss:0.25518\teval-mlogloss:0.24240\n",
      "[63]\ttrain-mlogloss:0.25360\teval-mlogloss:0.24103\n",
      "[64]\ttrain-mlogloss:0.25195\teval-mlogloss:0.23956\n",
      "[65]\ttrain-mlogloss:0.25044\teval-mlogloss:0.23816\n",
      "[66]\ttrain-mlogloss:0.24919\teval-mlogloss:0.23699\n",
      "[67]\ttrain-mlogloss:0.24784\teval-mlogloss:0.23587\n",
      "[68]\ttrain-mlogloss:0.24669\teval-mlogloss:0.23478\n",
      "[69]\ttrain-mlogloss:0.24534\teval-mlogloss:0.23376\n",
      "[70]\ttrain-mlogloss:0.24404\teval-mlogloss:0.23241\n",
      "[71]\ttrain-mlogloss:0.24269\teval-mlogloss:0.23127\n",
      "[72]\ttrain-mlogloss:0.24169\teval-mlogloss:0.23024\n",
      "[73]\ttrain-mlogloss:0.24062\teval-mlogloss:0.22943\n",
      "[74]\ttrain-mlogloss:0.23960\teval-mlogloss:0.22859\n",
      "[75]\ttrain-mlogloss:0.23871\teval-mlogloss:0.22772\n",
      "[76]\ttrain-mlogloss:0.23772\teval-mlogloss:0.22683\n",
      "[77]\ttrain-mlogloss:0.23678\teval-mlogloss:0.22617\n",
      "[78]\ttrain-mlogloss:0.23600\teval-mlogloss:0.22549\n",
      "[79]\ttrain-mlogloss:0.23513\teval-mlogloss:0.22460\n",
      "[80]\ttrain-mlogloss:0.23435\teval-mlogloss:0.22392\n",
      "[81]\ttrain-mlogloss:0.23354\teval-mlogloss:0.22337\n",
      "[82]\ttrain-mlogloss:0.23273\teval-mlogloss:0.22255\n",
      "[83]\ttrain-mlogloss:0.23191\teval-mlogloss:0.22192\n",
      "[84]\ttrain-mlogloss:0.23123\teval-mlogloss:0.22125\n",
      "[85]\ttrain-mlogloss:0.23035\teval-mlogloss:0.22078\n",
      "[86]\ttrain-mlogloss:0.22970\teval-mlogloss:0.22031\n",
      "[87]\ttrain-mlogloss:0.22903\teval-mlogloss:0.21982\n",
      "[88]\ttrain-mlogloss:0.22847\teval-mlogloss:0.21937\n",
      "[89]\ttrain-mlogloss:0.22770\teval-mlogloss:0.21887\n",
      "[90]\ttrain-mlogloss:0.22703\teval-mlogloss:0.21845\n",
      "[91]\ttrain-mlogloss:0.22642\teval-mlogloss:0.21792\n",
      "[92]\ttrain-mlogloss:0.22567\teval-mlogloss:0.21748\n",
      "[93]\ttrain-mlogloss:0.22489\teval-mlogloss:0.21688\n",
      "[94]\ttrain-mlogloss:0.22425\teval-mlogloss:0.21664\n",
      "[95]\ttrain-mlogloss:0.22368\teval-mlogloss:0.21614\n",
      "[96]\ttrain-mlogloss:0.22296\teval-mlogloss:0.21578\n",
      "[97]\ttrain-mlogloss:0.22242\teval-mlogloss:0.21538\n",
      "[98]\ttrain-mlogloss:0.22183\teval-mlogloss:0.21489\n",
      "[99]\ttrain-mlogloss:0.22117\teval-mlogloss:0.21448\n",
      "[100]\ttrain-mlogloss:0.22068\teval-mlogloss:0.21407\n",
      "[101]\ttrain-mlogloss:0.22018\teval-mlogloss:0.21361\n",
      "[102]\ttrain-mlogloss:0.21962\teval-mlogloss:0.21351\n",
      "[103]\ttrain-mlogloss:0.21875\teval-mlogloss:0.21324\n",
      "[104]\ttrain-mlogloss:0.21826\teval-mlogloss:0.21300\n",
      "[105]\ttrain-mlogloss:0.21796\teval-mlogloss:0.21285\n",
      "[106]\ttrain-mlogloss:0.21761\teval-mlogloss:0.21263\n",
      "[107]\ttrain-mlogloss:0.21694\teval-mlogloss:0.21221\n",
      "[108]\ttrain-mlogloss:0.21653\teval-mlogloss:0.21193\n",
      "[109]\ttrain-mlogloss:0.21600\teval-mlogloss:0.21183\n",
      "[110]\ttrain-mlogloss:0.21542\teval-mlogloss:0.21159\n",
      "[111]\ttrain-mlogloss:0.21485\teval-mlogloss:0.21145\n",
      "[112]\ttrain-mlogloss:0.21441\teval-mlogloss:0.21113\n",
      "[113]\ttrain-mlogloss:0.21400\teval-mlogloss:0.21105\n",
      "[114]\ttrain-mlogloss:0.21351\teval-mlogloss:0.21067\n",
      "[115]\ttrain-mlogloss:0.21291\teval-mlogloss:0.21049\n",
      "[116]\ttrain-mlogloss:0.21234\teval-mlogloss:0.21043\n",
      "[117]\ttrain-mlogloss:0.21173\teval-mlogloss:0.21020\n",
      "[118]\ttrain-mlogloss:0.21130\teval-mlogloss:0.21005\n",
      "[119]\ttrain-mlogloss:0.21078\teval-mlogloss:0.20993\n",
      "[120]\ttrain-mlogloss:0.21021\teval-mlogloss:0.20985\n",
      "[121]\ttrain-mlogloss:0.20954\teval-mlogloss:0.20963\n",
      "[122]\ttrain-mlogloss:0.20911\teval-mlogloss:0.20948\n",
      "[123]\ttrain-mlogloss:0.20876\teval-mlogloss:0.20927\n",
      "[124]\ttrain-mlogloss:0.20828\teval-mlogloss:0.20910\n",
      "[125]\ttrain-mlogloss:0.20785\teval-mlogloss:0.20886\n",
      "[126]\ttrain-mlogloss:0.20735\teval-mlogloss:0.20878\n",
      "[127]\ttrain-mlogloss:0.20676\teval-mlogloss:0.20850\n",
      "[128]\ttrain-mlogloss:0.20621\teval-mlogloss:0.20832\n",
      "[129]\ttrain-mlogloss:0.20575\teval-mlogloss:0.20811\n",
      "[130]\ttrain-mlogloss:0.20533\teval-mlogloss:0.20809\n",
      "[131]\ttrain-mlogloss:0.20488\teval-mlogloss:0.20812\n",
      "[132]\ttrain-mlogloss:0.20443\teval-mlogloss:0.20808\n",
      "[133]\ttrain-mlogloss:0.20396\teval-mlogloss:0.20801\n",
      "[134]\ttrain-mlogloss:0.20337\teval-mlogloss:0.20781\n",
      "[135]\ttrain-mlogloss:0.20301\teval-mlogloss:0.20763\n",
      "[136]\ttrain-mlogloss:0.20254\teval-mlogloss:0.20752\n",
      "[137]\ttrain-mlogloss:0.20209\teval-mlogloss:0.20737\n",
      "[138]\ttrain-mlogloss:0.20159\teval-mlogloss:0.20733\n",
      "[139]\ttrain-mlogloss:0.20136\teval-mlogloss:0.20730\n",
      "[140]\ttrain-mlogloss:0.20100\teval-mlogloss:0.20716\n",
      "[141]\ttrain-mlogloss:0.20060\teval-mlogloss:0.20712\n",
      "[142]\ttrain-mlogloss:0.20013\teval-mlogloss:0.20680\n",
      "[143]\ttrain-mlogloss:0.19976\teval-mlogloss:0.20668\n",
      "[144]\ttrain-mlogloss:0.19923\teval-mlogloss:0.20652\n",
      "[145]\ttrain-mlogloss:0.19878\teval-mlogloss:0.20659\n",
      "[146]\ttrain-mlogloss:0.19836\teval-mlogloss:0.20648\n",
      "[147]\ttrain-mlogloss:0.19794\teval-mlogloss:0.20644\n",
      "[148]\ttrain-mlogloss:0.19755\teval-mlogloss:0.20649\n",
      "[149]\ttrain-mlogloss:0.19718\teval-mlogloss:0.20654\n",
      "[150]\ttrain-mlogloss:0.19684\teval-mlogloss:0.20636\n",
      "[151]\ttrain-mlogloss:0.19641\teval-mlogloss:0.20627\n",
      "[152]\ttrain-mlogloss:0.19620\teval-mlogloss:0.20622\n",
      "[153]\ttrain-mlogloss:0.19586\teval-mlogloss:0.20626\n",
      "[154]\ttrain-mlogloss:0.19552\teval-mlogloss:0.20610\n",
      "[155]\ttrain-mlogloss:0.19525\teval-mlogloss:0.20602\n",
      "[156]\ttrain-mlogloss:0.19478\teval-mlogloss:0.20587\n",
      "[157]\ttrain-mlogloss:0.19438\teval-mlogloss:0.20589\n",
      "[158]\ttrain-mlogloss:0.19394\teval-mlogloss:0.20589\n",
      "[159]\ttrain-mlogloss:0.19355\teval-mlogloss:0.20583\n",
      "[160]\ttrain-mlogloss:0.19332\teval-mlogloss:0.20602\n",
      "[161]\ttrain-mlogloss:0.19288\teval-mlogloss:0.20597\n",
      "[162]\ttrain-mlogloss:0.19252\teval-mlogloss:0.20604\n",
      "[163]\ttrain-mlogloss:0.19219\teval-mlogloss:0.20597\n",
      "[164]\ttrain-mlogloss:0.19195\teval-mlogloss:0.20623\n",
      "[165]\ttrain-mlogloss:0.19149\teval-mlogloss:0.20637\n",
      "[166]\ttrain-mlogloss:0.19107\teval-mlogloss:0.20622\n",
      "[167]\ttrain-mlogloss:0.19077\teval-mlogloss:0.20636\n",
      "[168]\ttrain-mlogloss:0.19047\teval-mlogloss:0.20631\n",
      "[169]\ttrain-mlogloss:0.19011\teval-mlogloss:0.20631\n",
      "[170]\ttrain-mlogloss:0.18975\teval-mlogloss:0.20627\n",
      "[171]\ttrain-mlogloss:0.18939\teval-mlogloss:0.20616\n",
      "[172]\ttrain-mlogloss:0.18912\teval-mlogloss:0.20615\n",
      "[173]\ttrain-mlogloss:0.18894\teval-mlogloss:0.20616\n",
      "[174]\ttrain-mlogloss:0.18862\teval-mlogloss:0.20632\n",
      "[175]\ttrain-mlogloss:0.18832\teval-mlogloss:0.20621\n",
      "[176]\ttrain-mlogloss:0.18812\teval-mlogloss:0.20625\n",
      "[177]\ttrain-mlogloss:0.18788\teval-mlogloss:0.20626\n",
      "[178]\ttrain-mlogloss:0.18751\teval-mlogloss:0.20638\n",
      "[179]\ttrain-mlogloss:0.18722\teval-mlogloss:0.20649\n",
      "[180]\ttrain-mlogloss:0.18698\teval-mlogloss:0.20651\n",
      "[181]\ttrain-mlogloss:0.18670\teval-mlogloss:0.20667\n",
      "[182]\ttrain-mlogloss:0.18634\teval-mlogloss:0.20684\n",
      "[183]\ttrain-mlogloss:0.18600\teval-mlogloss:0.20699\n",
      "[184]\ttrain-mlogloss:0.18559\teval-mlogloss:0.20699\n",
      "[185]\ttrain-mlogloss:0.18533\teval-mlogloss:0.20701\n",
      "[186]\ttrain-mlogloss:0.18489\teval-mlogloss:0.20692\n",
      "[187]\ttrain-mlogloss:0.18470\teval-mlogloss:0.20693\n",
      "[188]\ttrain-mlogloss:0.18441\teval-mlogloss:0.20682\n",
      "[189]\ttrain-mlogloss:0.18404\teval-mlogloss:0.20692\n",
      "[190]\ttrain-mlogloss:0.18367\teval-mlogloss:0.20698\n",
      "[191]\ttrain-mlogloss:0.18332\teval-mlogloss:0.20677\n",
      "[192]\ttrain-mlogloss:0.18299\teval-mlogloss:0.20675\n",
      "[193]\ttrain-mlogloss:0.18268\teval-mlogloss:0.20677\n",
      "[194]\ttrain-mlogloss:0.18250\teval-mlogloss:0.20667\n",
      "[195]\ttrain-mlogloss:0.18220\teval-mlogloss:0.20668\n",
      "[196]\ttrain-mlogloss:0.18187\teval-mlogloss:0.20659\n",
      "[197]\ttrain-mlogloss:0.18152\teval-mlogloss:0.20676\n",
      "[198]\ttrain-mlogloss:0.18148\teval-mlogloss:0.20674\n",
      "[199]\ttrain-mlogloss:0.18123\teval-mlogloss:0.20664\n",
      "[200]\ttrain-mlogloss:0.18101\teval-mlogloss:0.20667\n",
      "[201]\ttrain-mlogloss:0.18084\teval-mlogloss:0.20680\n",
      "[202]\ttrain-mlogloss:0.18065\teval-mlogloss:0.20673\n",
      "[203]\ttrain-mlogloss:0.18043\teval-mlogloss:0.20664\n",
      "[204]\ttrain-mlogloss:0.18010\teval-mlogloss:0.20673\n",
      "[205]\ttrain-mlogloss:0.17987\teval-mlogloss:0.20679\n",
      "[206]\ttrain-mlogloss:0.17954\teval-mlogloss:0.20674\n",
      "[207]\ttrain-mlogloss:0.17921\teval-mlogloss:0.20674\n",
      "[208]\ttrain-mlogloss:0.17900\teval-mlogloss:0.20673\n",
      "[209]\ttrain-mlogloss:0.17875\teval-mlogloss:0.20682\n",
      "[210]\ttrain-mlogloss:0.17847\teval-mlogloss:0.20683\n",
      "[211]\ttrain-mlogloss:0.17814\teval-mlogloss:0.20676\n",
      "[212]\ttrain-mlogloss:0.17799\teval-mlogloss:0.20673\n",
      "[213]\ttrain-mlogloss:0.17764\teval-mlogloss:0.20669\n",
      "[214]\ttrain-mlogloss:0.17740\teval-mlogloss:0.20668\n",
      "[215]\ttrain-mlogloss:0.17694\teval-mlogloss:0.20664\n",
      "[216]\ttrain-mlogloss:0.17663\teval-mlogloss:0.20675\n",
      "[217]\ttrain-mlogloss:0.17643\teval-mlogloss:0.20680\n",
      "[218]\ttrain-mlogloss:0.17626\teval-mlogloss:0.20675\n",
      "[219]\ttrain-mlogloss:0.17602\teval-mlogloss:0.20694\n",
      "[220]\ttrain-mlogloss:0.17566\teval-mlogloss:0.20677\n",
      "[221]\ttrain-mlogloss:0.17549\teval-mlogloss:0.20673\n",
      "[222]\ttrain-mlogloss:0.17522\teval-mlogloss:0.20676\n",
      "[223]\ttrain-mlogloss:0.17478\teval-mlogloss:0.20684\n",
      "[224]\ttrain-mlogloss:0.17459\teval-mlogloss:0.20692\n",
      "[225]\ttrain-mlogloss:0.17433\teval-mlogloss:0.20705\n",
      "[226]\ttrain-mlogloss:0.17414\teval-mlogloss:0.20690\n",
      "[227]\ttrain-mlogloss:0.17378\teval-mlogloss:0.20700\n",
      "[228]\ttrain-mlogloss:0.17345\teval-mlogloss:0.20691\n",
      "[229]\ttrain-mlogloss:0.17320\teval-mlogloss:0.20691\n",
      "[230]\ttrain-mlogloss:0.17303\teval-mlogloss:0.20693\n",
      "[231]\ttrain-mlogloss:0.17282\teval-mlogloss:0.20702\n",
      "[232]\ttrain-mlogloss:0.17259\teval-mlogloss:0.20691\n",
      "[233]\ttrain-mlogloss:0.17228\teval-mlogloss:0.20685\n",
      "[234]\ttrain-mlogloss:0.17203\teval-mlogloss:0.20674\n",
      "[235]\ttrain-mlogloss:0.17176\teval-mlogloss:0.20703\n",
      "[236]\ttrain-mlogloss:0.17146\teval-mlogloss:0.20705\n",
      "[237]\ttrain-mlogloss:0.17120\teval-mlogloss:0.20717\n",
      "[238]\ttrain-mlogloss:0.17094\teval-mlogloss:0.20724\n",
      "[239]\ttrain-mlogloss:0.17088\teval-mlogloss:0.20725\n",
      "[240]\ttrain-mlogloss:0.17055\teval-mlogloss:0.20729\n",
      "[241]\ttrain-mlogloss:0.17041\teval-mlogloss:0.20732\n",
      "[242]\ttrain-mlogloss:0.17015\teval-mlogloss:0.20742\n",
      "[243]\ttrain-mlogloss:0.16999\teval-mlogloss:0.20743\n",
      "[244]\ttrain-mlogloss:0.16983\teval-mlogloss:0.20740\n",
      "[245]\ttrain-mlogloss:0.16952\teval-mlogloss:0.20743\n",
      "[246]\ttrain-mlogloss:0.16923\teval-mlogloss:0.20759\n",
      "[247]\ttrain-mlogloss:0.16894\teval-mlogloss:0.20755\n",
      "[248]\ttrain-mlogloss:0.16891\teval-mlogloss:0.20752\n",
      "[249]\ttrain-mlogloss:0.16869\teval-mlogloss:0.20756\n",
      "[250]\ttrain-mlogloss:0.16852\teval-mlogloss:0.20750\n",
      "[251]\ttrain-mlogloss:0.16836\teval-mlogloss:0.20749\n",
      "[252]\ttrain-mlogloss:0.16790\teval-mlogloss:0.20748\n",
      "[253]\ttrain-mlogloss:0.16765\teval-mlogloss:0.20744\n",
      "[254]\ttrain-mlogloss:0.16737\teval-mlogloss:0.20748\n",
      "[255]\ttrain-mlogloss:0.16716\teval-mlogloss:0.20748\n",
      "[256]\ttrain-mlogloss:0.16691\teval-mlogloss:0.20739\n",
      "[257]\ttrain-mlogloss:0.16658\teval-mlogloss:0.20735\n",
      "[258]\ttrain-mlogloss:0.16621\teval-mlogloss:0.20750\n",
      "[259]\ttrain-mlogloss:0.16609\teval-mlogloss:0.20751\n",
      "xgb now score is: [2.4929980950802566, 2.1850327644497156, 2.371474780375138, 2.617098775021732]\n",
      "[0]\ttrain-mlogloss:0.67128\teval-mlogloss:0.67080\n",
      "[1]\ttrain-mlogloss:0.65071\teval-mlogloss:0.64987\n",
      "[2]\ttrain-mlogloss:0.63138\teval-mlogloss:0.63042\n",
      "[3]\ttrain-mlogloss:0.61287\teval-mlogloss:0.61182\n",
      "[4]\ttrain-mlogloss:0.59513\teval-mlogloss:0.59377\n",
      "[5]\ttrain-mlogloss:0.57858\teval-mlogloss:0.57697\n",
      "[6]\ttrain-mlogloss:0.56283\teval-mlogloss:0.56078\n",
      "[7]\ttrain-mlogloss:0.54775\teval-mlogloss:0.54550\n",
      "[8]\ttrain-mlogloss:0.53340\teval-mlogloss:0.53090\n",
      "[9]\ttrain-mlogloss:0.52004\teval-mlogloss:0.51736\n",
      "[10]\ttrain-mlogloss:0.50730\teval-mlogloss:0.50431\n",
      "[11]\ttrain-mlogloss:0.49534\teval-mlogloss:0.49202\n",
      "[12]\ttrain-mlogloss:0.48390\teval-mlogloss:0.48036\n",
      "[13]\ttrain-mlogloss:0.47299\teval-mlogloss:0.46918\n",
      "[14]\ttrain-mlogloss:0.46255\teval-mlogloss:0.45835\n",
      "[15]\ttrain-mlogloss:0.45247\teval-mlogloss:0.44824\n",
      "[16]\ttrain-mlogloss:0.44295\teval-mlogloss:0.43835\n",
      "[17]\ttrain-mlogloss:0.43381\teval-mlogloss:0.42894\n",
      "[18]\ttrain-mlogloss:0.42503\teval-mlogloss:0.42003\n",
      "[19]\ttrain-mlogloss:0.41675\teval-mlogloss:0.41165\n",
      "[20]\ttrain-mlogloss:0.40871\teval-mlogloss:0.40356\n",
      "[21]\ttrain-mlogloss:0.40093\teval-mlogloss:0.39573\n",
      "[22]\ttrain-mlogloss:0.39369\teval-mlogloss:0.38829\n",
      "[23]\ttrain-mlogloss:0.38663\teval-mlogloss:0.38110\n",
      "[24]\ttrain-mlogloss:0.37987\teval-mlogloss:0.37429\n",
      "[25]\ttrain-mlogloss:0.37350\teval-mlogloss:0.36769\n",
      "[26]\ttrain-mlogloss:0.36737\teval-mlogloss:0.36150\n",
      "[27]\ttrain-mlogloss:0.36140\teval-mlogloss:0.35531\n",
      "[28]\ttrain-mlogloss:0.35580\teval-mlogloss:0.34956\n",
      "[29]\ttrain-mlogloss:0.35036\teval-mlogloss:0.34407\n",
      "[30]\ttrain-mlogloss:0.34510\teval-mlogloss:0.33883\n",
      "[31]\ttrain-mlogloss:0.33999\teval-mlogloss:0.33365\n",
      "[32]\ttrain-mlogloss:0.33534\teval-mlogloss:0.32888\n",
      "[33]\ttrain-mlogloss:0.33058\teval-mlogloss:0.32391\n",
      "[34]\ttrain-mlogloss:0.32628\teval-mlogloss:0.31961\n",
      "[35]\ttrain-mlogloss:0.32217\teval-mlogloss:0.31546\n",
      "[36]\ttrain-mlogloss:0.31805\teval-mlogloss:0.31129\n",
      "[37]\ttrain-mlogloss:0.31428\teval-mlogloss:0.30740\n",
      "[38]\ttrain-mlogloss:0.31069\teval-mlogloss:0.30360\n",
      "[39]\ttrain-mlogloss:0.30714\teval-mlogloss:0.29997\n",
      "[40]\ttrain-mlogloss:0.30368\teval-mlogloss:0.29639\n",
      "[41]\ttrain-mlogloss:0.30040\teval-mlogloss:0.29310\n",
      "[42]\ttrain-mlogloss:0.29729\teval-mlogloss:0.28991\n",
      "[43]\ttrain-mlogloss:0.29421\teval-mlogloss:0.28684\n",
      "[44]\ttrain-mlogloss:0.29128\teval-mlogloss:0.28396\n",
      "[45]\ttrain-mlogloss:0.28840\teval-mlogloss:0.28107\n",
      "[46]\ttrain-mlogloss:0.28565\teval-mlogloss:0.27842\n",
      "[47]\ttrain-mlogloss:0.28303\teval-mlogloss:0.27572\n",
      "[48]\ttrain-mlogloss:0.28064\teval-mlogloss:0.27336\n",
      "[49]\ttrain-mlogloss:0.27827\teval-mlogloss:0.27110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\xgboost\\core.py:94: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain-mlogloss:0.27588\teval-mlogloss:0.26866\n",
      "[51]\ttrain-mlogloss:0.27356\teval-mlogloss:0.26653\n",
      "[52]\ttrain-mlogloss:0.27153\teval-mlogloss:0.26454\n",
      "[53]\ttrain-mlogloss:0.26939\teval-mlogloss:0.26247\n",
      "[54]\ttrain-mlogloss:0.26747\teval-mlogloss:0.26075\n",
      "[55]\ttrain-mlogloss:0.26560\teval-mlogloss:0.25886\n",
      "[56]\ttrain-mlogloss:0.26367\teval-mlogloss:0.25684\n",
      "[57]\ttrain-mlogloss:0.26186\teval-mlogloss:0.25494\n",
      "[58]\ttrain-mlogloss:0.25997\teval-mlogloss:0.25319\n",
      "[59]\ttrain-mlogloss:0.25820\teval-mlogloss:0.25159\n",
      "[60]\ttrain-mlogloss:0.25657\teval-mlogloss:0.25007\n",
      "[61]\ttrain-mlogloss:0.25505\teval-mlogloss:0.24863\n",
      "[62]\ttrain-mlogloss:0.25356\teval-mlogloss:0.24720\n",
      "[63]\ttrain-mlogloss:0.25213\teval-mlogloss:0.24586\n",
      "[64]\ttrain-mlogloss:0.25067\teval-mlogloss:0.24435\n",
      "[65]\ttrain-mlogloss:0.24934\teval-mlogloss:0.24304\n",
      "[66]\ttrain-mlogloss:0.24788\teval-mlogloss:0.24175\n",
      "[67]\ttrain-mlogloss:0.24673\teval-mlogloss:0.24054\n",
      "[68]\ttrain-mlogloss:0.24552\teval-mlogloss:0.23942\n",
      "[69]\ttrain-mlogloss:0.24439\teval-mlogloss:0.23834\n",
      "[70]\ttrain-mlogloss:0.24325\teval-mlogloss:0.23734\n",
      "[71]\ttrain-mlogloss:0.24219\teval-mlogloss:0.23629\n",
      "[72]\ttrain-mlogloss:0.24122\teval-mlogloss:0.23546\n",
      "[73]\ttrain-mlogloss:0.24027\teval-mlogloss:0.23461\n",
      "[74]\ttrain-mlogloss:0.23940\teval-mlogloss:0.23371\n",
      "[75]\ttrain-mlogloss:0.23830\teval-mlogloss:0.23274\n",
      "[76]\ttrain-mlogloss:0.23705\teval-mlogloss:0.23165\n",
      "[77]\ttrain-mlogloss:0.23606\teval-mlogloss:0.23094\n",
      "[78]\ttrain-mlogloss:0.23506\teval-mlogloss:0.23027\n",
      "[79]\ttrain-mlogloss:0.23408\teval-mlogloss:0.22964\n",
      "[80]\ttrain-mlogloss:0.23328\teval-mlogloss:0.22883\n",
      "[81]\ttrain-mlogloss:0.23247\teval-mlogloss:0.22812\n",
      "[82]\ttrain-mlogloss:0.23157\teval-mlogloss:0.22759\n",
      "[83]\ttrain-mlogloss:0.23078\teval-mlogloss:0.22706\n",
      "[84]\ttrain-mlogloss:0.22982\teval-mlogloss:0.22629\n",
      "[85]\ttrain-mlogloss:0.22913\teval-mlogloss:0.22578\n",
      "[86]\ttrain-mlogloss:0.22833\teval-mlogloss:0.22524\n",
      "[87]\ttrain-mlogloss:0.22751\teval-mlogloss:0.22466\n",
      "[88]\ttrain-mlogloss:0.22682\teval-mlogloss:0.22406\n",
      "[89]\ttrain-mlogloss:0.22615\teval-mlogloss:0.22345\n",
      "[90]\ttrain-mlogloss:0.22556\teval-mlogloss:0.22320\n",
      "[91]\ttrain-mlogloss:0.22494\teval-mlogloss:0.22282\n",
      "[92]\ttrain-mlogloss:0.22428\teval-mlogloss:0.22242\n",
      "[93]\ttrain-mlogloss:0.22369\teval-mlogloss:0.22200\n",
      "[94]\ttrain-mlogloss:0.22312\teval-mlogloss:0.22157\n",
      "[95]\ttrain-mlogloss:0.22252\teval-mlogloss:0.22105\n",
      "[96]\ttrain-mlogloss:0.22190\teval-mlogloss:0.22081\n",
      "[97]\ttrain-mlogloss:0.22138\teval-mlogloss:0.22048\n",
      "[98]\ttrain-mlogloss:0.22087\teval-mlogloss:0.22015\n",
      "[99]\ttrain-mlogloss:0.22037\teval-mlogloss:0.21975\n",
      "[100]\ttrain-mlogloss:0.21982\teval-mlogloss:0.21940\n",
      "[101]\ttrain-mlogloss:0.21929\teval-mlogloss:0.21914\n",
      "[102]\ttrain-mlogloss:0.21866\teval-mlogloss:0.21876\n",
      "[103]\ttrain-mlogloss:0.21807\teval-mlogloss:0.21842\n",
      "[104]\ttrain-mlogloss:0.21763\teval-mlogloss:0.21835\n",
      "[105]\ttrain-mlogloss:0.21707\teval-mlogloss:0.21803\n",
      "[106]\ttrain-mlogloss:0.21658\teval-mlogloss:0.21777\n",
      "[107]\ttrain-mlogloss:0.21609\teval-mlogloss:0.21754\n",
      "[108]\ttrain-mlogloss:0.21566\teval-mlogloss:0.21736\n",
      "[109]\ttrain-mlogloss:0.21500\teval-mlogloss:0.21705\n",
      "[110]\ttrain-mlogloss:0.21444\teval-mlogloss:0.21679\n",
      "[111]\ttrain-mlogloss:0.21401\teval-mlogloss:0.21656\n",
      "[112]\ttrain-mlogloss:0.21354\teval-mlogloss:0.21637\n",
      "[113]\ttrain-mlogloss:0.21310\teval-mlogloss:0.21618\n",
      "[114]\ttrain-mlogloss:0.21252\teval-mlogloss:0.21586\n",
      "[115]\ttrain-mlogloss:0.21208\teval-mlogloss:0.21596\n",
      "[116]\ttrain-mlogloss:0.21157\teval-mlogloss:0.21588\n",
      "[117]\ttrain-mlogloss:0.21103\teval-mlogloss:0.21562\n",
      "[118]\ttrain-mlogloss:0.21068\teval-mlogloss:0.21545\n",
      "[119]\ttrain-mlogloss:0.21017\teval-mlogloss:0.21528\n",
      "[120]\ttrain-mlogloss:0.20963\teval-mlogloss:0.21506\n",
      "[121]\ttrain-mlogloss:0.20915\teval-mlogloss:0.21482\n",
      "[122]\ttrain-mlogloss:0.20880\teval-mlogloss:0.21462\n",
      "[123]\ttrain-mlogloss:0.20827\teval-mlogloss:0.21467\n",
      "[124]\ttrain-mlogloss:0.20789\teval-mlogloss:0.21460\n",
      "[125]\ttrain-mlogloss:0.20731\teval-mlogloss:0.21426\n",
      "[126]\ttrain-mlogloss:0.20682\teval-mlogloss:0.21434\n",
      "[127]\ttrain-mlogloss:0.20629\teval-mlogloss:0.21410\n",
      "[128]\ttrain-mlogloss:0.20575\teval-mlogloss:0.21409\n",
      "[129]\ttrain-mlogloss:0.20520\teval-mlogloss:0.21410\n",
      "[130]\ttrain-mlogloss:0.20472\teval-mlogloss:0.21390\n",
      "[131]\ttrain-mlogloss:0.20439\teval-mlogloss:0.21379\n",
      "[132]\ttrain-mlogloss:0.20397\teval-mlogloss:0.21360\n",
      "[133]\ttrain-mlogloss:0.20343\teval-mlogloss:0.21346\n",
      "[134]\ttrain-mlogloss:0.20286\teval-mlogloss:0.21340\n",
      "[135]\ttrain-mlogloss:0.20244\teval-mlogloss:0.21325\n",
      "[136]\ttrain-mlogloss:0.20201\teval-mlogloss:0.21321\n",
      "[137]\ttrain-mlogloss:0.20177\teval-mlogloss:0.21314\n",
      "[138]\ttrain-mlogloss:0.20128\teval-mlogloss:0.21321\n",
      "[139]\ttrain-mlogloss:0.20101\teval-mlogloss:0.21320\n",
      "[140]\ttrain-mlogloss:0.20054\teval-mlogloss:0.21298\n",
      "[141]\ttrain-mlogloss:0.20007\teval-mlogloss:0.21293\n",
      "[142]\ttrain-mlogloss:0.19965\teval-mlogloss:0.21287\n",
      "[143]\ttrain-mlogloss:0.19923\teval-mlogloss:0.21287\n",
      "[144]\ttrain-mlogloss:0.19890\teval-mlogloss:0.21291\n",
      "[145]\ttrain-mlogloss:0.19862\teval-mlogloss:0.21305\n",
      "[146]\ttrain-mlogloss:0.19824\teval-mlogloss:0.21277\n",
      "[147]\ttrain-mlogloss:0.19787\teval-mlogloss:0.21262\n",
      "[148]\ttrain-mlogloss:0.19753\teval-mlogloss:0.21240\n",
      "[149]\ttrain-mlogloss:0.19710\teval-mlogloss:0.21253\n",
      "[150]\ttrain-mlogloss:0.19675\teval-mlogloss:0.21257\n",
      "[151]\ttrain-mlogloss:0.19653\teval-mlogloss:0.21251\n",
      "[152]\ttrain-mlogloss:0.19612\teval-mlogloss:0.21246\n",
      "[153]\ttrain-mlogloss:0.19561\teval-mlogloss:0.21225\n",
      "[154]\ttrain-mlogloss:0.19528\teval-mlogloss:0.21222\n",
      "[155]\ttrain-mlogloss:0.19492\teval-mlogloss:0.21203\n",
      "[156]\ttrain-mlogloss:0.19447\teval-mlogloss:0.21195\n",
      "[157]\ttrain-mlogloss:0.19419\teval-mlogloss:0.21196\n",
      "[158]\ttrain-mlogloss:0.19380\teval-mlogloss:0.21200\n",
      "[159]\ttrain-mlogloss:0.19346\teval-mlogloss:0.21199\n",
      "[160]\ttrain-mlogloss:0.19309\teval-mlogloss:0.21213\n",
      "[161]\ttrain-mlogloss:0.19274\teval-mlogloss:0.21202\n",
      "[162]\ttrain-mlogloss:0.19251\teval-mlogloss:0.21201\n",
      "[163]\ttrain-mlogloss:0.19210\teval-mlogloss:0.21206\n",
      "[164]\ttrain-mlogloss:0.19196\teval-mlogloss:0.21205\n",
      "[165]\ttrain-mlogloss:0.19171\teval-mlogloss:0.21212\n",
      "[166]\ttrain-mlogloss:0.19131\teval-mlogloss:0.21214\n",
      "[167]\ttrain-mlogloss:0.19099\teval-mlogloss:0.21213\n",
      "[168]\ttrain-mlogloss:0.19078\teval-mlogloss:0.21214\n",
      "[169]\ttrain-mlogloss:0.19042\teval-mlogloss:0.21199\n",
      "[170]\ttrain-mlogloss:0.19017\teval-mlogloss:0.21202\n",
      "[171]\ttrain-mlogloss:0.18984\teval-mlogloss:0.21200\n",
      "[172]\ttrain-mlogloss:0.18949\teval-mlogloss:0.21195\n",
      "[173]\ttrain-mlogloss:0.18931\teval-mlogloss:0.21202\n",
      "[174]\ttrain-mlogloss:0.18888\teval-mlogloss:0.21189\n",
      "[175]\ttrain-mlogloss:0.18850\teval-mlogloss:0.21175\n",
      "[176]\ttrain-mlogloss:0.18811\teval-mlogloss:0.21172\n",
      "[177]\ttrain-mlogloss:0.18771\teval-mlogloss:0.21192\n",
      "[178]\ttrain-mlogloss:0.18730\teval-mlogloss:0.21191\n",
      "[179]\ttrain-mlogloss:0.18702\teval-mlogloss:0.21208\n",
      "[180]\ttrain-mlogloss:0.18676\teval-mlogloss:0.21217\n",
      "[181]\ttrain-mlogloss:0.18651\teval-mlogloss:0.21222\n",
      "[182]\ttrain-mlogloss:0.18616\teval-mlogloss:0.21217\n",
      "[183]\ttrain-mlogloss:0.18592\teval-mlogloss:0.21220\n",
      "[184]\ttrain-mlogloss:0.18561\teval-mlogloss:0.21227\n",
      "[185]\ttrain-mlogloss:0.18533\teval-mlogloss:0.21227\n",
      "[186]\ttrain-mlogloss:0.18493\teval-mlogloss:0.21225\n",
      "[187]\ttrain-mlogloss:0.18452\teval-mlogloss:0.21243\n",
      "[188]\ttrain-mlogloss:0.18417\teval-mlogloss:0.21237\n",
      "[189]\ttrain-mlogloss:0.18388\teval-mlogloss:0.21230\n",
      "[190]\ttrain-mlogloss:0.18373\teval-mlogloss:0.21235\n",
      "[191]\ttrain-mlogloss:0.18332\teval-mlogloss:0.21246\n",
      "[192]\ttrain-mlogloss:0.18300\teval-mlogloss:0.21256\n",
      "[193]\ttrain-mlogloss:0.18261\teval-mlogloss:0.21237\n",
      "[194]\ttrain-mlogloss:0.18245\teval-mlogloss:0.21243\n",
      "[195]\ttrain-mlogloss:0.18218\teval-mlogloss:0.21264\n",
      "[196]\ttrain-mlogloss:0.18193\teval-mlogloss:0.21266\n",
      "[197]\ttrain-mlogloss:0.18153\teval-mlogloss:0.21265\n",
      "[198]\ttrain-mlogloss:0.18131\teval-mlogloss:0.21269\n",
      "[199]\ttrain-mlogloss:0.18104\teval-mlogloss:0.21266\n",
      "[200]\ttrain-mlogloss:0.18085\teval-mlogloss:0.21269\n",
      "[201]\ttrain-mlogloss:0.18061\teval-mlogloss:0.21288\n",
      "[202]\ttrain-mlogloss:0.18038\teval-mlogloss:0.21277\n",
      "[203]\ttrain-mlogloss:0.18016\teval-mlogloss:0.21264\n",
      "[204]\ttrain-mlogloss:0.17982\teval-mlogloss:0.21270\n",
      "[205]\ttrain-mlogloss:0.17956\teval-mlogloss:0.21269\n",
      "[206]\ttrain-mlogloss:0.17929\teval-mlogloss:0.21289\n",
      "[207]\ttrain-mlogloss:0.17898\teval-mlogloss:0.21314\n",
      "[208]\ttrain-mlogloss:0.17865\teval-mlogloss:0.21327\n",
      "[209]\ttrain-mlogloss:0.17834\teval-mlogloss:0.21350\n",
      "[210]\ttrain-mlogloss:0.17800\teval-mlogloss:0.21355\n",
      "[211]\ttrain-mlogloss:0.17774\teval-mlogloss:0.21370\n",
      "[212]\ttrain-mlogloss:0.17746\teval-mlogloss:0.21367\n",
      "[213]\ttrain-mlogloss:0.17719\teval-mlogloss:0.21366\n",
      "[214]\ttrain-mlogloss:0.17702\teval-mlogloss:0.21371\n",
      "[215]\ttrain-mlogloss:0.17680\teval-mlogloss:0.21365\n",
      "[216]\ttrain-mlogloss:0.17653\teval-mlogloss:0.21371\n",
      "[217]\ttrain-mlogloss:0.17635\teval-mlogloss:0.21357\n",
      "[218]\ttrain-mlogloss:0.17618\teval-mlogloss:0.21357\n",
      "[219]\ttrain-mlogloss:0.17589\teval-mlogloss:0.21363\n",
      "[220]\ttrain-mlogloss:0.17568\teval-mlogloss:0.21371\n",
      "[221]\ttrain-mlogloss:0.17543\teval-mlogloss:0.21382\n",
      "[222]\ttrain-mlogloss:0.17518\teval-mlogloss:0.21383\n",
      "[223]\ttrain-mlogloss:0.17491\teval-mlogloss:0.21378\n",
      "[224]\ttrain-mlogloss:0.17464\teval-mlogloss:0.21393\n",
      "[225]\ttrain-mlogloss:0.17439\teval-mlogloss:0.21395\n",
      "[226]\ttrain-mlogloss:0.17418\teval-mlogloss:0.21400\n",
      "[227]\ttrain-mlogloss:0.17395\teval-mlogloss:0.21403\n",
      "[228]\ttrain-mlogloss:0.17366\teval-mlogloss:0.21395\n",
      "[229]\ttrain-mlogloss:0.17329\teval-mlogloss:0.21400\n",
      "[230]\ttrain-mlogloss:0.17300\teval-mlogloss:0.21408\n",
      "[231]\ttrain-mlogloss:0.17268\teval-mlogloss:0.21417\n",
      "[232]\ttrain-mlogloss:0.17236\teval-mlogloss:0.21404\n",
      "[233]\ttrain-mlogloss:0.17209\teval-mlogloss:0.21392\n",
      "[234]\ttrain-mlogloss:0.17179\teval-mlogloss:0.21414\n",
      "[235]\ttrain-mlogloss:0.17152\teval-mlogloss:0.21395\n",
      "[236]\ttrain-mlogloss:0.17129\teval-mlogloss:0.21420\n",
      "[237]\ttrain-mlogloss:0.17125\teval-mlogloss:0.21420\n",
      "[238]\ttrain-mlogloss:0.17109\teval-mlogloss:0.21432\n",
      "[239]\ttrain-mlogloss:0.17091\teval-mlogloss:0.21427\n",
      "[240]\ttrain-mlogloss:0.17057\teval-mlogloss:0.21424\n",
      "[241]\ttrain-mlogloss:0.17035\teval-mlogloss:0.21419\n",
      "[242]\ttrain-mlogloss:0.17018\teval-mlogloss:0.21416\n",
      "[243]\ttrain-mlogloss:0.16998\teval-mlogloss:0.21403\n",
      "[244]\ttrain-mlogloss:0.16982\teval-mlogloss:0.21409\n",
      "[245]\ttrain-mlogloss:0.16950\teval-mlogloss:0.21415\n",
      "[246]\ttrain-mlogloss:0.16926\teval-mlogloss:0.21434\n",
      "[247]\ttrain-mlogloss:0.16908\teval-mlogloss:0.21447\n",
      "[248]\ttrain-mlogloss:0.16884\teval-mlogloss:0.21436\n",
      "[249]\ttrain-mlogloss:0.16857\teval-mlogloss:0.21444\n",
      "[250]\ttrain-mlogloss:0.16847\teval-mlogloss:0.21455\n",
      "[251]\ttrain-mlogloss:0.16819\teval-mlogloss:0.21454\n",
      "[252]\ttrain-mlogloss:0.16797\teval-mlogloss:0.21459\n",
      "[253]\ttrain-mlogloss:0.16764\teval-mlogloss:0.21468\n",
      "[254]\ttrain-mlogloss:0.16745\teval-mlogloss:0.21479\n",
      "[255]\ttrain-mlogloss:0.16720\teval-mlogloss:0.21496\n",
      "[256]\ttrain-mlogloss:0.16704\teval-mlogloss:0.21496\n",
      "[257]\ttrain-mlogloss:0.16693\teval-mlogloss:0.21489\n",
      "[258]\ttrain-mlogloss:0.16673\teval-mlogloss:0.21485\n",
      "[259]\ttrain-mlogloss:0.16645\teval-mlogloss:0.21493\n",
      "[260]\ttrain-mlogloss:0.16628\teval-mlogloss:0.21493\n",
      "[261]\ttrain-mlogloss:0.16610\teval-mlogloss:0.21515\n",
      "[262]\ttrain-mlogloss:0.16594\teval-mlogloss:0.21516\n",
      "[263]\ttrain-mlogloss:0.16575\teval-mlogloss:0.21532\n",
      "[264]\ttrain-mlogloss:0.16553\teval-mlogloss:0.21527\n",
      "[265]\ttrain-mlogloss:0.16533\teval-mlogloss:0.21513\n",
      "[266]\ttrain-mlogloss:0.16515\teval-mlogloss:0.21522\n",
      "[267]\ttrain-mlogloss:0.16487\teval-mlogloss:0.21543\n",
      "[268]\ttrain-mlogloss:0.16457\teval-mlogloss:0.21539\n",
      "[269]\ttrain-mlogloss:0.16441\teval-mlogloss:0.21542\n",
      "[270]\ttrain-mlogloss:0.16422\teval-mlogloss:0.21523\n",
      "[271]\ttrain-mlogloss:0.16407\teval-mlogloss:0.21509\n",
      "[272]\ttrain-mlogloss:0.16388\teval-mlogloss:0.21515\n",
      "[273]\ttrain-mlogloss:0.16368\teval-mlogloss:0.21519\n",
      "[274]\ttrain-mlogloss:0.16349\teval-mlogloss:0.21522\n",
      "[275]\ttrain-mlogloss:0.16331\teval-mlogloss:0.21519\n",
      "[276]\ttrain-mlogloss:0.16313\teval-mlogloss:0.21536\n",
      "xgb now score is: [2.4929980950802566, 2.1850327644497156, 2.371474780375138, 2.617098775021732, 2.689968637730926]\n",
      "xgb_score_list: [2.4929980950802566, 2.1850327644497156, 2.371474780375138, 2.617098775021732, 2.689968637730926]\n",
      "xgb_score_mean: 2.4713146105315538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Doog\\anaconda3\\envs\\ali1\\lib\\site-packages\\xgboost\\core.py:94: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  UserWarning\n"
     ]
    }
   ],
   "source": [
    "# 训练模型获取stacking特征\n",
    "\n",
    "clf_list = clf_list\n",
    "columns_list = []\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "for clf in clf_list:\n",
    "    train_data, test_data, clf_name = clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并stacking特征 保存导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并所有特征\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)\n",
    "\n",
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col\n",
    "\n",
    "# 获取数据ID以及特征标签LABEL\n",
    "df_train_all['user_id'] = all_data_test[~all_data_test['label'].isna()]['user_id']\n",
    "df_test_all['user_id'] = all_data_test[all_data_test['label'].isna()]['user_id']\n",
    "df_train_all['label'] = all_data_test[~all_data_test['label'].isna()]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('../data/train_all.csv',header=True,index=False)\n",
    "df_test_all.to_csv('../data/test_all.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
